#!/usr/bin/env python3
"""
Quick Test Validator
ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ê²€ì¦ ë° ì‹¤í–‰ ìƒíƒœ í™•ì¸
"""

import os
import json
import subprocess
import time
from datetime import datetime
from pathlib import Path

def check_environment():
    """í™˜ê²½ ì„¤ì • í™•ì¸"""
    print("ğŸ” í™˜ê²½ ì„¤ì • í™•ì¸ ì¤‘...")
    
    checks = {
        'python': False,
        'node': False,
        'playwright': False,
        'frontend_deps': False,
        'test_files': False
    }
    
    # Python í™•ì¸
    try:
        result = subprocess.run(['python3', '--version'], capture_output=True, text=True)
        if result.returncode == 0:
            checks['python'] = True
            print(f"âœ… Python: {result.stdout.strip()}")
    except:
        print("âŒ Python3 not found")
    
    # Node.js í™•ì¸
    try:
        result = subprocess.run(['node', '--version'], capture_output=True, text=True)
        if result.returncode == 0:
            checks['node'] = True
            print(f"âœ… Node.js: {result.stdout.strip()}")
    except:
        print("âŒ Node.js not found")
    
    # Playwright í™•ì¸
    try:
        result = subprocess.run(['python3', '-c', 'import playwright; print("Playwright available")'], 
                              capture_output=True, text=True)
        if result.returncode == 0:
            checks['playwright'] = True
            print("âœ… Playwright available")
    except:
        print("âŒ Playwright not available")
    
    # Frontend dependencies í™•ì¸
    if os.path.exists('frontend/node_modules'):
        checks['frontend_deps'] = True
        print("âœ… Frontend dependencies installed")
    else:
        print("âŒ Frontend dependencies missing")
    
    # Test files í™•ì¸
    test_files = [
        'tests/scenarios/admin-scenarios.yml',
        'tests/scenarios/secretary-scenarios.yml',
        'tests/scenarios/evaluator-scenarios.yml',
        'tests/scenario-data/test-accounts.json'
    ]
    
    if all(os.path.exists(f) for f in test_files):
        checks['test_files'] = True
        print("âœ… Test configuration files present")
    else:
        print("âŒ Some test files missing")
    
    return checks

def run_basic_validation():
    """ê¸°ë³¸ ê²€ì¦ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ§ª ê¸°ë³¸ ê²€ì¦ í…ŒìŠ¤íŠ¸ ì‹¤í–‰...")
    
    validations = {
        'scenario_files_valid': False,
        'test_data_valid': False,
        'frontend_build': False,
        'system_connectivity': False
    }
    
    # ì‹œë‚˜ë¦¬ì˜¤ íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬
    try:
        import yaml
        scenario_files = [
            'tests/scenarios/admin-scenarios.yml',
            'tests/scenarios/secretary-scenarios.yml',
            'tests/scenarios/evaluator-scenarios.yml'
        ]
        
        for file_path in scenario_files:
            if os.path.exists(file_path):
                with open(file_path, 'r', encoding='utf-8') as f:
                    yaml.safe_load(f)
        
        validations['scenario_files_valid'] = True
        print("âœ… ì‹œë‚˜ë¦¬ì˜¤ íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬ í†µê³¼")
    except Exception as e:
        print(f"âŒ ì‹œë‚˜ë¦¬ì˜¤ íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬ ì‹¤íŒ¨: {str(e)}")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬
    try:
        test_data_files = [
            'tests/scenario-data/test-accounts.json',
            'tests/scenario-data/sample-companies.json'
        ]
        
        for file_path in test_data_files:
            if os.path.exists(file_path):
                with open(file_path, 'r', encoding='utf-8') as f:
                    json.load(f)
        
        validations['test_data_valid'] = True
        print("âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬ í†µê³¼")
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬ ì‹¤íŒ¨: {str(e)}")
    
    # Frontend ë¹Œë“œ í…ŒìŠ¤íŠ¸
    try:
        if os.path.exists('frontend/package.json'):
            print("ğŸ”¨ Frontend ë¹Œë“œ í…ŒìŠ¤íŠ¸ ì¤‘...")
            result = subprocess.run(
                ['npm', 'run', 'build'], 
                cwd='frontend', 
                capture_output=True, 
                text=True, 
                timeout=120
            )
            
            if result.returncode == 0:
                validations['frontend_build'] = True
                print("âœ… Frontend ë¹Œë“œ ì„±ê³µ")
            else:
                print(f"âŒ Frontend ë¹Œë“œ ì‹¤íŒ¨: {result.stderr[:200]}...")
    except Exception as e:
        print(f"âŒ Frontend ë¹Œë“œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
    
    # ì‹œìŠ¤í…œ ì—°ê²°ì„± í™•ì¸
    try:
        # í¬íŠ¸ í™•ì¸
        import socket
        
        def check_port(host, port):
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.settimeout(1)
            result = sock.connect_ex((host, port))
            sock.close()
            return result == 0
        
        frontend_port = check_port('localhost', 3000)
        backend_port = check_port('localhost', 8002)
        
        if frontend_port or backend_port:
            validations['system_connectivity'] = True
            print(f"âœ… ì‹œìŠ¤í…œ ì—°ê²°ì„± í™•ì¸ (Frontend: {frontend_port}, Backend: {backend_port})")
        else:
            print("âš ï¸  ì„œë¹„ìŠ¤ê°€ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ (ì •ìƒì ì¸ ìƒí™©ì¼ ìˆ˜ ìˆìŒ)")
            validations['system_connectivity'] = True  # ì„œë¹„ìŠ¤ ë¯¸ì‹¤í–‰ì€ ì •ìƒ ìƒí™©
            
    except Exception as e:
        print(f"âŒ ì‹œìŠ¤í…œ ì—°ê²°ì„± í™•ì¸ ì‹¤íŒ¨: {str(e)}")
    
    return validations

def run_quick_scenario_test():
    """ë¹ ë¥¸ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""
    print("\nğŸš€ ë¹ ë¥¸ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ ì‹¤í–‰...")
    
    # ê°„ë‹¨í•œ ì‹œë‚˜ë¦¬ì˜¤ ìœ íš¨ì„± í…ŒìŠ¤íŠ¸
    test_results = {
        'admin_scenario_structure': False,
        'secretary_scenario_structure': False,
        'evaluator_scenario_structure': False,
        'test_data_structure': False
    }
    
    try:
        import yaml
        
        # Admin ì‹œë‚˜ë¦¬ì˜¤ êµ¬ì¡° í™•ì¸
        with open('tests/scenarios/admin-scenarios.yml', 'r', encoding='utf-8') as f:
            admin_data = yaml.safe_load(f)
            if 'scenarios' in admin_data and len(admin_data['scenarios']) > 0:
                test_results['admin_scenario_structure'] = True
                print(f"âœ… Admin ì‹œë‚˜ë¦¬ì˜¤: {len(admin_data['scenarios'])}ê°œ í™•ì¸")
        
        # Secretary ì‹œë‚˜ë¦¬ì˜¤ êµ¬ì¡° í™•ì¸
        with open('tests/scenarios/secretary-scenarios.yml', 'r', encoding='utf-8') as f:
            secretary_data = yaml.safe_load(f)
            if 'scenarios' in secretary_data and len(secretary_data['scenarios']) > 0:
                test_results['secretary_scenario_structure'] = True
                print(f"âœ… Secretary ì‹œë‚˜ë¦¬ì˜¤: {len(secretary_data['scenarios'])}ê°œ í™•ì¸")
        
        # Evaluator ì‹œë‚˜ë¦¬ì˜¤ êµ¬ì¡° í™•ì¸
        with open('tests/scenarios/evaluator-scenarios.yml', 'r', encoding='utf-8') as f:
            evaluator_data = yaml.safe_load(f)
            if 'scenarios' in evaluator_data and len(evaluator_data['scenarios']) > 0:
                test_results['evaluator_scenario_structure'] = True
                print(f"âœ… Evaluator ì‹œë‚˜ë¦¬ì˜¤: {len(evaluator_data['scenarios'])}ê°œ í™•ì¸")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° êµ¬ì¡° í™•ì¸
        with open('tests/scenario-data/test-accounts.json', 'r', encoding='utf-8') as f:
            test_accounts = json.load(f)
            if 'test_accounts' in test_accounts:
                accounts = test_accounts['test_accounts']
                admin_count = 1 if 'admin' in accounts else 0
                secretary_count = 1 if 'secretary' in accounts else 0
                evaluator_count = len(accounts.get('evaluators', []))
                
                test_results['test_data_structure'] = True
                print(f"âœ… í…ŒìŠ¤íŠ¸ ê³„ì •: Admin({admin_count}), Secretary({secretary_count}), Evaluator({evaluator_count})")
    
    except Exception as e:
        print(f"âŒ ì‹œë‚˜ë¦¬ì˜¤ êµ¬ì¡° í™•ì¸ ì‹¤íŒ¨: {str(e)}")
    
    return test_results

def run_performance_quick_check():
    """ë¹ ë¥¸ ì„±ëŠ¥ í™•ì¸"""
    print("\nâš¡ ë¹ ë¥¸ ì„±ëŠ¥ í™•ì¸...")
    
    perf_results = {
        'build_size_check': False,
        'dependency_check': False,
        'file_structure_check': False
    }
    
    # Build í¬ê¸° í™•ì¸
    try:
        if os.path.exists('frontend/build'):
            import shutil
            build_size = shutil.disk_usage('frontend/build').used
            build_size_mb = build_size / (1024 * 1024)
            
            perf_results['build_size_check'] = build_size_mb < 50  # 50MB ì´í•˜
            print(f"âœ… Build í¬ê¸°: {build_size_mb:.1f}MB {'(ì ì •)' if build_size_mb < 50 else '(í° í¸)'}")
        else:
            print("âš ï¸  Build í´ë” ì—†ìŒ")
    except Exception as e:
        print(f"âŒ Build í¬ê¸° í™•ì¸ ì‹¤íŒ¨: {str(e)}")
    
    # ì˜ì¡´ì„± í™•ì¸
    try:
        if os.path.exists('frontend/package.json'):
            with open('frontend/package.json', 'r') as f:
                package_data = json.load(f)
                deps = package_data.get('dependencies', {})
                dev_deps = package_data.get('devDependencies', {})
                
                total_deps = len(deps) + len(dev_deps)
                perf_results['dependency_check'] = total_deps < 100  # 100ê°œ ì´í•˜
                print(f"âœ… ì˜ì¡´ì„±: {total_deps}ê°œ {'(ì ì •)' if total_deps < 100 else '(ë§ì€ í¸)'}")
    except Exception as e:
        print(f"âŒ ì˜ì¡´ì„± í™•ì¸ ì‹¤íŒ¨: {str(e)}")
    
    # íŒŒì¼ êµ¬ì¡° í™•ì¸
    try:
        test_files_count = len(list(Path('tests').rglob('*.py'))) + len(list(Path('tests').rglob('*.yml')))
        src_files_count = len(list(Path('frontend/src').rglob('*.js'))) if os.path.exists('frontend/src') else 0
        
        perf_results['file_structure_check'] = test_files_count > 5 and src_files_count > 5
        print(f"âœ… íŒŒì¼ êµ¬ì¡°: í…ŒìŠ¤íŠ¸({test_files_count}), ì†ŒìŠ¤({src_files_count})")
    except Exception as e:
        print(f"âŒ íŒŒì¼ êµ¬ì¡° í™•ì¸ ì‹¤íŒ¨: {str(e)}")
    
    return perf_results

def generate_quick_report(env_checks, validations, scenario_tests, perf_results):
    """ë¹ ë¥¸ ë³´ê³ ì„œ ìƒì„±"""
    print("\nğŸ“‹ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ ìƒì„±...")
    
    # ì „ì²´ ì ìˆ˜ ê³„ì‚°
    total_checks = sum([
        sum(env_checks.values()),
        sum(validations.values()),
        sum(scenario_tests.values()),
        sum(perf_results.values())
    ])
    
    max_checks = sum([
        len(env_checks),
        len(validations),
        len(scenario_tests),
        len(perf_results)
    ])
    
    success_rate = (total_checks / max_checks * 100) if max_checks > 0 else 0
    
    # ë“±ê¸‰ ì‚°ì •
    if success_rate >= 95:
        grade = "A+"
    elif success_rate >= 90:
        grade = "A"
    elif success_rate >= 85:
        grade = "B+"
    elif success_rate >= 80:
        grade = "B"
    elif success_rate >= 75:
        grade = "C+"
    else:
        grade = "C"
    
    report = {
        'timestamp': datetime.now().isoformat(),
        'test_type': 'quick_validation',
        'overall_score': success_rate,
        'grade': grade,
        'checks_passed': total_checks,
        'total_checks': max_checks,
        'categories': {
            'environment': env_checks,
            'validations': validations,
            'scenarios': scenario_tests,
            'performance': perf_results
        },
        'recommendations': []
    }
    
    # ê¶Œê³ ì‚¬í•­ ìƒì„±
    if not env_checks['playwright']:
        report['recommendations'].append("Playwright ì„¤ì¹˜ í•„ìš”: pip install playwright && playwright install")
    
    if not env_checks['frontend_deps']:
        report['recommendations'].append("Frontend ì˜ì¡´ì„± ì„¤ì¹˜ í•„ìš”: cd frontend && npm install")
    
    if not validations['frontend_build']:
        report['recommendations'].append("Frontend ë¹Œë“œ í™•ì¸ í•„ìš”: cd frontend && npm run build")
    
    if success_rate < 80:
        report['recommendations'].append("ì „ì²´ ì‹œìŠ¤í…œ ê²€í†  ë° ëˆ„ë½ëœ ì„¤ì • í™•ì¸ í•„ìš”")
    
    # JSON ë³´ê³ ì„œ ì €ì¥
    report_file = f"quick_test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    # ì½˜ì†” ì¶œë ¥
    print("\n" + "="*60)
    print("ğŸ¯ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ê²€ì¦ ê²°ê³¼")
    print("="*60)
    print(f"ğŸ“Š ì „ì²´ ì ìˆ˜: {success_rate:.1f}% (ë“±ê¸‰: {grade})")
    print(f"âœ… í†µê³¼: {total_checks}/{max_checks}")
    print(f"ğŸ“„ ë³´ê³ ì„œ: {report_file}")
    
    if report['recommendations']:
        print("\nğŸ’¡ ê¶Œê³ ì‚¬í•­:")
        for i, rec in enumerate(report['recommendations'], 1):
            print(f"  {i}. {rec}")
    
    print("="*60)
    
    return report_file, success_rate

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ Quick Test Validator")
    print("Ultra Comprehensive Test ì¤€ë¹„ ìƒíƒœ í™•ì¸")
    print("="*60)
    
    # 1. í™˜ê²½ í™•ì¸
    env_checks = check_environment()
    
    # 2. ê¸°ë³¸ ê²€ì¦
    validations = run_basic_validation()
    
    # 3. ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸
    scenario_tests = run_quick_scenario_test()
    
    # 4. ì„±ëŠ¥ í™•ì¸
    perf_results = run_performance_quick_check()
    
    # 5. ë³´ê³ ì„œ ìƒì„±
    report_file, success_rate = generate_quick_report(
        env_checks, validations, scenario_tests, perf_results
    )
    
    # ìµœì¢… ìƒíƒœ ì¶œë ¥
    if success_rate >= 85:
        print("\nğŸ‰ ì‹œìŠ¤í…œì´ Ultra Comprehensive Test ì¤€ë¹„ ì™„ë£Œ!")
        print("python3 ultra_comprehensive_test_runner.py ì‹¤í–‰ ê°€ëŠ¥")
    elif success_rate >= 70:
        print("\nâš ï¸  ì¼ë¶€ ì„¤ì •ì´ ëˆ„ë½ë˜ì—ˆì§€ë§Œ ê¸°ë³¸ í…ŒìŠ¤íŠ¸ëŠ” ê°€ëŠ¥")
        print("ê¶Œê³ ì‚¬í•­ì„ í™•ì¸í•˜ì—¬ ê°œì„  í›„ ì¬ì‹¤í–‰ ê¶Œì¥")
    else:
        print("\nâŒ ì£¼ìš” ì„¤ì •ì´ ëˆ„ë½ë¨. ê¶Œê³ ì‚¬í•­ì„ ë”°ë¼ ì„¤ì • ì™„ë£Œ í›„ ì¬ì‹¤í–‰")
    
    return success_rate >= 70

if __name__ == "__main__":
    main()