#!/usr/bin/env python3
"""
CLI í†µí•© í…ŒìŠ¤íŠ¸
===============

ì‹¤ì œ CLI ëª…ë ¹ì–´ë“¤ì„ ì‹¤í–‰í•˜ì—¬ ì „ì²´ ì›Œí¬í”Œë¡œìš°ê°€ ì •ìƒ ë™ì‘í•˜ëŠ”ì§€ ê²€ì¦
"""

import os
import sys
import json
import subprocess
import tempfile
from pathlib import Path
from datetime import datetime
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger(__name__)

class CLIIntegrationTest:
    """CLI í†µí•© í…ŒìŠ¤íŠ¸"""
    
    def __init__(self):
        self.test_dir = Path(tempfile.mkdtemp(prefix='cli_test_'))
        self.project_root = Path(__file__).parent.parent
        self.results = []
        
        # í™˜ê²½ ì„¤ì •
        os.environ['PYTHONPATH'] = str(self.project_root)
        
    def run_cli_command(self, args, timeout=30):
        """CLI ëª…ë ¹ì–´ ì‹¤í–‰"""
        cmd = [sys.executable, '-m', 'universal_port_manager.cli'] + args
        
        try:
            result = subprocess.run(
                cmd,
                cwd=self.test_dir,
                capture_output=True,
                text=True,
                timeout=timeout,
                env=os.environ
            )
            
            return {
                'success': result.returncode == 0,
                'stdout': result.stdout,
                'stderr': result.stderr,
                'returncode': result.returncode
            }
            
        except subprocess.TimeoutExpired:
            return {
                'success': False,
                'error': 'Command timeout'
            }
        except Exception as e:
            return {
                'success': False,
                'error': str(e)
            }
    
    def test_workflow_scenario(self):
        """ì „ì²´ ì›Œí¬í”Œë¡œìš° ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ”„ ì „ì²´ ì›Œí¬í”Œë¡œìš° ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸")
        
        workflow_steps = [
            {
                'name': 'doctor_check',
                'cmd': ['doctor'],
                'description': 'ì‹œìŠ¤í…œ ì§„ë‹¨'
            },
            {
                'name': 'port_scan',
                'cmd': ['scan', '--range', '3000-3010'],
                'description': 'í¬íŠ¸ ìŠ¤ìº”'
            },
            {
                'name': 'port_allocation',
                'cmd': ['--project', 'test-workflow', 'allocate', 'frontend', 'backend', '--dry-run'],
                'description': 'í¬íŠ¸ í• ë‹¹ (ë¯¸ë¦¬ë³´ê¸°)'
            },
            {
                'name': 'actual_allocation',
                'cmd': ['--project', 'test-workflow', 'allocate', 'frontend', 'backend'],
                'description': 'ì‹¤ì œ í¬íŠ¸ í• ë‹¹'
            },
            {
                'name': 'status_check',
                'cmd': ['--project', 'test-workflow', 'status'],
                'description': 'ìƒíƒœ í™•ì¸'
            },
            {
                'name': 'generate_configs',
                'cmd': ['--project', 'test-workflow', 'generate'],
                'description': 'ì„¤ì • íŒŒì¼ ìƒì„±'
            },
            {
                'name': 'cleanup',
                'cmd': ['--project', 'test-workflow', 'cleanup'],
                'description': 'ì •ë¦¬ ì‘ì—…'
            }
        ]
        
        workflow_results = []
        
        for step in workflow_steps:
            logger.info(f"  ğŸ”¹ {step['description']}")
            
            result = self.run_cli_command(step['cmd'])
            
            step_result = {
                'name': step['name'],
                'description': step['description'],
                'success': result['success'],
                'has_output': len(result.get('stdout', '')) > 0
            }
            
            if not result['success']:
                step_result['error'] = result.get('error') or result.get('stderr')
            
            workflow_results.append(step_result)
            
            # ì‹¤íŒ¨í•œ ë‹¨ê³„ê°€ ìˆìœ¼ë©´ ë¡œê·¸ ì¶œë ¥
            if not result['success']:
                logger.warning(f"    âŒ ì‹¤íŒ¨: {step_result.get('error')}")
            else:
                logger.info(f"    âœ… ì„±ê³µ")
        
        # ì›Œí¬í”Œë¡œìš° í‰ê°€
        successful_steps = sum(1 for step in workflow_results if step['success'])
        total_steps = len(workflow_steps)
        
        return {
            'test': 'workflow_scenario',
            'success': successful_steps >= total_steps * 0.8,  # 80% ì´ìƒ ì„±ê³µ
            'details': {
                'total_steps': total_steps,
                'successful_steps': successful_steps,
                'success_rate': successful_steps / total_steps * 100,
                'step_results': workflow_results
            },
            'timestamp': datetime.now().isoformat()
        }
    
    def test_multi_project_scenario(self):
        """ë‹¤ì¤‘ í”„ë¡œì íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ”„ ë‹¤ì¤‘ í”„ë¡œì íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸")
        
        projects = ['project-alpha', 'project-beta', 'project-gamma']
        project_results = []
        
        for project in projects:
            logger.info(f"  ğŸ”¹ í”„ë¡œì íŠ¸: {project}")
            
            # ê° í”„ë¡œì íŠ¸ì— í¬íŠ¸ í• ë‹¹
            result = self.run_cli_command([
                '--project', project, 
                'allocate', 'frontend', 'backend'
            ])
            
            project_result = {
                'project': project,
                'allocation_success': result['success'],
                'has_output': len(result.get('stdout', '')) > 0
            }
            
            if result['success']:
                # ìƒíƒœ í™•ì¸
                status_result = self.run_cli_command([
                    '--project', project, 'status'
                ])
                project_result['status_check'] = status_result['success']
                
                logger.info(f"    âœ… {project} í• ë‹¹ ë° ìƒíƒœ í™•ì¸ ì„±ê³µ")
            else:
                project_result['error'] = result.get('error') or result.get('stderr')
                logger.warning(f"    âŒ {project} í• ë‹¹ ì‹¤íŒ¨")
            
            project_results.append(project_result)
        
        # ë‹¤ì¤‘ í”„ë¡œì íŠ¸ ê²°ê³¼ í‰ê°€
        successful_projects = sum(
            1 for p in project_results 
            if p['allocation_success']
        )
        
        return {
            'test': 'multi_project_scenario',
            'success': successful_projects >= len(projects) * 0.7,  # 70% ì´ìƒ ì„±ê³µ
            'details': {
                'total_projects': len(projects),
                'successful_projects': successful_projects,
                'project_results': project_results
            },
            'timestamp': datetime.now().isoformat()
        }
    
    def test_error_handling(self):
        """ì˜¤ë¥˜ ì²˜ë¦¬ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ”„ ì˜¤ë¥˜ ì²˜ë¦¬ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸")
        
        error_scenarios = [
            {
                'name': 'invalid_port_range',
                'cmd': ['scan', '--range', 'invalid'],
                'should_fail': True,
                'description': 'ì˜ëª»ëœ í¬íŠ¸ ë²”ìœ„'
            },
            {
                'name': 'invalid_json',
                'cmd': ['allocate', 'frontend', '--preferred-ports', '{invalid}'],
                'should_fail': True,
                'description': 'ì˜ëª»ëœ JSON í˜•ì‹'
            },
            {
                'name': 'unknown_command',
                'cmd': ['unknown_command'],
                'should_fail': True,
                'description': 'ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ëª…ë ¹ì–´'
            },
            {
                'name': 'status_without_allocation',
                'cmd': ['--project', 'nonexistent', 'status'],
                'should_fail': False,  # í• ë‹¹ì´ ì—†ì–´ë„ ìƒíƒœëŠ” í™•ì¸ ê°€ëŠ¥í•´ì•¼ í•¨
                'description': 'í• ë‹¹ ì—†ëŠ” í”„ë¡œì íŠ¸ ìƒíƒœ í™•ì¸'
            }
        ]
        
        error_results = []
        
        for scenario in error_scenarios:
            logger.info(f"  ğŸ”¹ {scenario['description']}")
            
            result = self.run_cli_command(scenario['cmd'])
            
            # ì˜ˆìƒ ê²°ê³¼ì™€ ì‹¤ì œ ê²°ê³¼ ë¹„êµ
            expected_success = not scenario['should_fail']
            actual_success = result['success']
            correct_handling = (expected_success == actual_success)
            
            error_result = {
                'name': scenario['name'],
                'description': scenario['description'],
                'should_fail': scenario['should_fail'],
                'actual_success': actual_success,
                'correct_handling': correct_handling
            }
            
            if correct_handling:
                logger.info(f"    âœ… ì˜¬ë°”ë¥¸ ì²˜ë¦¬")
            else:
                logger.warning(f"    âŒ ì˜ˆìƒê³¼ ë‹¤ë¥¸ ê²°ê³¼")
                error_result['error'] = result.get('error') or result.get('stderr')
            
            error_results.append(error_result)
        
        # ì˜¤ë¥˜ ì²˜ë¦¬ í‰ê°€
        correct_handling_count = sum(
            1 for r in error_results 
            if r['correct_handling']
        )
        
        return {
            'test': 'error_handling',
            'success': correct_handling_count == len(error_scenarios),
            'details': {
                'total_scenarios': len(error_scenarios),
                'correct_handling': correct_handling_count,
                'scenario_results': error_results
            },
            'timestamp': datetime.now().isoformat()
        }
    
    def test_dependency_scenarios(self):
        """ì˜ì¡´ì„± ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ”„ ì˜ì¡´ì„± ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸")
        
        dependency_tests = [
            {
                'name': 'doctor_basic',
                'cmd': ['doctor'],
                'description': 'ê¸°ë³¸ ì§„ë‹¨'
            },
            {
                'name': 'doctor_minimal',
                'cmd': ['doctor', '--group', 'minimal'],
                'description': 'ìµœì†Œ ì˜ì¡´ì„± ì§„ë‹¨'
            },
            {
                'name': 'doctor_full',
                'cmd': ['doctor', '--group', 'full'],
                'description': 'ì „ì²´ ì˜ì¡´ì„± ì§„ë‹¨'
            },
            {
                'name': 'doctor_report',
                'cmd': ['doctor', '--report'],
                'description': 'ìƒì„¸ ë³´ê³ ì„œ'
            }
        ]
        
        dependency_results = []
        
        for test in dependency_tests:
            logger.info(f"  ğŸ”¹ {test['description']}")
            
            result = self.run_cli_command(test['cmd'])
            
            test_result = {
                'name': test['name'],
                'description': test['description'],
                'success': result['success'],
                'has_dependency_info': 'ì˜ì¡´ì„±' in result.get('stdout', '') or 'dependency' in result.get('stdout', '').lower()
            }
            
            if result['success']:
                logger.info(f"    âœ… ì„±ê³µ")
            else:
                test_result['error'] = result.get('error') or result.get('stderr')
                logger.warning(f"    âŒ ì‹¤íŒ¨")
            
            dependency_results.append(test_result)
        
        # ì˜ì¡´ì„± í…ŒìŠ¤íŠ¸ í‰ê°€
        successful_tests = sum(
            1 for t in dependency_results 
            if t['success']
        )
        
        return {
            'test': 'dependency_scenarios',
            'success': successful_tests >= len(dependency_tests) * 0.8,
            'details': {
                'total_tests': len(dependency_tests),
                'successful_tests': successful_tests,
                'test_results': dependency_results
            },
            'timestamp': datetime.now().isoformat()
        }
    
    def run_all_tests(self):
        """ëª¨ë“  CLI í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("ğŸš€ CLI í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
        logger.info("=" * 50)
        
        tests = [
            self.test_workflow_scenario,
            self.test_multi_project_scenario,
            self.test_error_handling,
            self.test_dependency_scenarios
        ]
        
        for test in tests:
            try:
                logger.info(f"\nğŸ“‹ {test.__name__} ì‹¤í–‰ ì¤‘...")
                result = test()
                self.results.append(result)
                
                if result['success']:
                    logger.info(f"âœ… {test.__name__} ì„±ê³µ")
                else:
                    logger.error(f"âŒ {test.__name__} ì‹¤íŒ¨")
                    
            except Exception as e:
                self.results.append({
                    'test': test.__name__,
                    'success': False,
                    'error': str(e),
                    'timestamp': datetime.now().isoformat()
                })
                logger.error(f"ğŸ’¥ {test.__name__} ì˜ˆì™¸: {e}")
        
        return self.generate_report()
    
    def generate_report(self):
        """í†µí•© í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ ìƒì„±"""
        successful_tests = sum(1 for r in self.results if r['success'])
        total_tests = len(self.results)
        success_rate = (successful_tests / total_tests * 100) if total_tests > 0 else 0
        
        report = {
            'overall_success': success_rate >= 80,  # 80% ì´ìƒ ì„±ê³µ
            'summary': {
                'total_tests': total_tests,
                'successful_tests': successful_tests,
                'failed_tests': total_tests - successful_tests,
                'success_rate': success_rate
            },
            'test_results': self.results,
            'environment': {
                'test_directory': str(self.test_dir),
                'project_root': str(self.project_root)
            },
            'timestamp': datetime.now().isoformat()
        }
        
        # ë³´ê³ ì„œ íŒŒì¼ ì €ì¥
        report_file = Path.cwd() / f'cli_integration_test_report_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ“Š CLI í†µí•© í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ ì €ì¥: {report_file}")
        
        return report
    
    def cleanup(self):
        """í…ŒìŠ¤íŠ¸ ì •ë¦¬"""
        try:
            import shutil
            if self.test_dir.exists():
                shutil.rmtree(self.test_dir)
        except Exception as e:
            logger.warning(f"ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")


def main():
    """CLI í†µí•© í…ŒìŠ¤íŠ¸ ë©”ì¸"""
    test_suite = CLIIntegrationTest()
    
    try:
        report = test_suite.run_all_tests()
        
        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "=" * 60)
        print("ğŸ¯ CLI í†µí•© í…ŒìŠ¤íŠ¸ ê²°ê³¼")
        print("=" * 60)
        
        if report['overall_success']:
            print("âœ… ì „ì²´ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
        else:
            print("âŒ ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
        
        summary = report['summary']
        print(f"\nğŸ“Š ìš”ì•½:")
        print(f"  ì „ì²´ í…ŒìŠ¤íŠ¸: {summary['total_tests']}ê°œ")
        print(f"  ì„±ê³µ: {summary['successful_tests']}ê°œ")
        print(f"  ì‹¤íŒ¨: {summary['failed_tests']}ê°œ")
        print(f"  ì„±ê³µë¥ : {summary['success_rate']:.1f}%")
        
        # ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ ìƒì„¸
        failed_tests = [r for r in report['test_results'] if not r['success']]
        if failed_tests:
            print(f"\nâŒ ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸:")
            for test in failed_tests:
                print(f"  - {test['test']}: {test.get('error', 'Unknown error')}")
        else:
            print(f"\nğŸ‰ ëª¨ë“  CLI í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µí–ˆìŠµë‹ˆë‹¤!")
        
        return 0 if report['overall_success'] else 1
        
    finally:
        test_suite.cleanup()


if __name__ == '__main__':
    sys.exit(main())