#!/usr/bin/env python3
"""
AI ëª¨ë¸ ê´€ë¦¬ ì‹œìŠ¤í…œ - ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
MongoDB ì„±ëŠ¥ ë° ì¿¼ë¦¬ ìµœì í™” ë¶„ì„
"""

import asyncio
import motor.motor_asyncio
import pymongo
import time
import statistics
from typing import Dict, List, Any
import logging
from datetime import datetime, timedelta
import random
import json
from dataclasses import dataclass
import os
from dotenv import load_dotenv

load_dotenv('.env.performance')

@dataclass
class DBPerformanceResult:
    """DB ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼"""
    operation: str
    execution_times: List[float]
    documents_processed: int
    errors: List[str]
    avg_time: float
    max_time: float
    min_time: float

class DatabasePerformanceTest:
    """ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.mongodb_url = os.getenv('MONGODB_URL', 'mongodb://localhost:27017/online_evaluation_test')
        self.client = None
        self.db = None
        self.results: List[DBPerformanceResult] = []
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
    async def setup_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„¤ì •"""
        self.client = motor.motor_asyncio.AsyncIOMotorClient(self.mongodb_url)
        self.db = self.client.get_default_database()
        
        # ì—°ê²° í…ŒìŠ¤íŠ¸
        try:
            await self.client.admin.command('ismaster')
            self.logger.info("MongoDB ì—°ê²° ì„±ê³µ")
        except Exception as e:
            self.logger.error(f"MongoDB ì—°ê²° ì‹¤íŒ¨: {e}")
            raise
            
    async def cleanup_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì •ë¦¬"""
        if self.client:
            self.client.close()
            
    async def create_test_data(self, num_models: int = 1000):
        """í…ŒìŠ¤íŠ¸ìš© AI ëª¨ë¸ ë°ì´í„° ìƒì„±"""
        self.logger.info(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì¤‘: {num_models}ê°œ ëª¨ë¸")
        
        # ê¸°ì¡´ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •ë¦¬
        await self.db.ai_models_test.delete_many({"model_id": {"$regex": "^test-"}})
        
        providers = ["openai", "anthropic", "google", "novita", "local"]
        model_names = ["gpt-3.5-turbo", "gpt-4", "claude-3-haiku", "claude-3-sonnet", 
                      "gemini-pro", "llama-2-70b", "custom-model"]
        
        test_models = []
        for i in range(num_models):
            provider = random.choice(providers)
            model_name = random.choice(model_names)
            
            model = {
                "model_id": f"test-model-{i:05d}",
                "provider": provider,
                "model_name": model_name,
                "display_name": f"Test Model {i}",
                "api_endpoint": f"https://api.{provider}.com/v1/chat/completions",
                "input_cost_per_token": round(random.uniform(0.0001, 0.01), 6),
                "output_cost_per_token": round(random.uniform(0.0002, 0.02), 6),
                "max_tokens": random.choice([2048, 4096, 8192, 16384]),
                "capabilities": random.sample([
                    "text-generation", "analysis", "korean", "evaluation", 
                    "multilingual", "coding", "reasoning"
                ], k=random.randint(2, 5)),
                "quality_score": round(random.uniform(0.5, 1.0), 2),
                "speed_score": round(random.uniform(0.3, 1.0), 2),
                "cost_efficiency": round(random.uniform(0.4, 1.0), 2),
                "reliability_score": round(random.uniform(0.6, 1.0), 2),
                "is_default": False,
                "is_active": random.choice([True, False]),
                "created_at": datetime.now() - timedelta(days=random.randint(1, 365)),
                "updated_at": datetime.now() - timedelta(days=random.randint(0, 30))
            }
            test_models.append(model)
            
        # ë°°ì¹˜ ì‚½ì…
        start_time = time.time()
        await self.db.ai_models_test.insert_many(test_models)
        end_time = time.time()
        
        self.logger.info(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì™„ë£Œ: {end_time - start_time:.2f}ì´ˆ")
        
        # ì¸ë±ìŠ¤ ìƒì„±
        await self._create_indexes()
        
    async def _create_indexes(self):
        """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ì¸ë±ìŠ¤ ìƒì„±"""
        self.logger.info("ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
        
        indexes = [
            [("model_id", 1)],  # ë‹¨ì¼ í•„ë“œ ì¸ë±ìŠ¤
            [("provider", 1), ("is_active", 1)],  # ë³µí•© ì¸ë±ìŠ¤
            [("quality_score", -1)],  # ë‚´ë¦¼ì°¨ìˆœ ì¸ë±ìŠ¤
            [("created_at", -1)],  # ì‹œê°„ ê¸°ë°˜ ì¸ë±ìŠ¤
            [("capabilities", 1)],  # ë°°ì—´ í•„ë“œ ì¸ë±ìŠ¤
            [("display_name", "text")]  # í…ìŠ¤íŠ¸ ì¸ë±ìŠ¤
        ]
        
        for index in indexes:
            try:
                await self.db.ai_models_test.create_index(index)
            except Exception as e:
                self.logger.warning(f"ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {index} - {e}")
                
    async def test_basic_operations(self) -> List[DBPerformanceResult]:
        """ê¸°ë³¸ CRUD ì—°ì‚° ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        results = []
        
        # 1. ë‹¨ì¼ ë¬¸ì„œ ì¡°íšŒ (ì¸ë±ìŠ¤ ì‚¬ìš©)
        result = await self._test_operation(
            "single_find_indexed",
            self._single_find_indexed,
            iterations=1000
        )
        results.append(result)
        
        # 2. ë‹¨ì¼ ë¬¸ì„œ ì¡°íšŒ (ì¸ë±ìŠ¤ ë¯¸ì‚¬ìš©)
        result = await self._test_operation(
            "single_find_unindexed", 
            self._single_find_unindexed,
            iterations=100
        )
        results.append(result)
        
        # 3. ë‹¤ì¤‘ ë¬¸ì„œ ì¡°íšŒ
        result = await self._test_operation(
            "multiple_find",
            self._multiple_find,
            iterations=100
        )
        results.append(result)
        
        # 4. ì§‘ê³„ íŒŒì´í”„ë¼ì¸
        result = await self._test_operation(
            "aggregation_pipeline",
            self._aggregation_pipeline,
            iterations=50
        )
        results.append(result)
        
        # 5. ë¬¸ì„œ ì‚½ì…
        result = await self._test_operation(
            "document_insert",
            self._document_insert,
            iterations=100
        )
        results.append(result)
        
        # 6. ë¬¸ì„œ ì—…ë°ì´íŠ¸
        result = await self._test_operation(
            "document_update",
            self._document_update,
            iterations=100
        )
        results.append(result)
        
        # 7. ë¬¸ì„œ ì‚­ì œ
        result = await self._test_operation(
            "document_delete",
            self._document_delete,
            iterations=100
        )
        results.append(result)
        
        return results
        
    async def _test_operation(self, operation_name: str, operation_func, iterations: int) -> DBPerformanceResult:
        """ê°œë³„ ì—°ì‚° ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        self.logger.info(f"í…ŒìŠ¤íŠ¸ ì¤‘: {operation_name} ({iterations}íšŒ)")
        
        execution_times = []
        errors = []
        documents_processed = 0
        
        for i in range(iterations):
            try:
                start_time = time.time()
                doc_count = await operation_func()
                end_time = time.time()
                
                execution_times.append((end_time - start_time) * 1000)  # ms
                documents_processed += doc_count
                
            except Exception as e:
                errors.append(f"Iteration {i}: {str(e)}")
                
        # í†µê³„ ê³„ì‚°
        if execution_times:
            avg_time = statistics.mean(execution_times)
            max_time = max(execution_times)
            min_time = min(execution_times)
        else:
            avg_time = max_time = min_time = 0
            
        return DBPerformanceResult(
            operation=operation_name,
            execution_times=execution_times,
            documents_processed=documents_processed,
            errors=errors,
            avg_time=avg_time,
            max_time=max_time,
            min_time=min_time
        )
        
    async def _single_find_indexed(self) -> int:
        """ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•œ ë‹¨ì¼ ë¬¸ì„œ ì¡°íšŒ"""
        model_id = f"test-model-{random.randint(0, 999):05d}"
        doc = await self.db.ai_models_test.find_one({"model_id": model_id})
        return 1 if doc else 0
        
    async def _single_find_unindexed(self) -> int:
        """ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ë‹¨ì¼ ë¬¸ì„œ ì¡°íšŒ"""
        quality_score = round(random.uniform(0.5, 1.0), 2)
        doc = await self.db.ai_models_test.find_one({"quality_score": quality_score})
        return 1 if doc else 0
        
    async def _multiple_find(self) -> int:
        """ë‹¤ì¤‘ ë¬¸ì„œ ì¡°íšŒ"""
        provider = random.choice(["openai", "anthropic", "google", "novita"])
        cursor = self.db.ai_models_test.find({"provider": provider}).limit(50)
        docs = await cursor.to_list(length=50)
        return len(docs)
        
    async def _aggregation_pipeline(self) -> int:
        """ì§‘ê³„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        pipeline = [
            {"$match": {"is_active": True}},
            {"$group": {
                "_id": "$provider",
                "avg_quality": {"$avg": "$quality_score"},
                "avg_speed": {"$avg": "$speed_score"},
                "count": {"$sum": 1}
            }},
            {"$sort": {"avg_quality": -1}}
        ]
        
        cursor = self.db.ai_models_test.aggregate(pipeline)
        results = await cursor.to_list(length=None)
        return len(results)
        
    async def _document_insert(self) -> int:
        """ë¬¸ì„œ ì‚½ì…"""
        doc = {
            "model_id": f"perf-test-{int(time.time() * 1000000)}",
            "provider": "test",
            "model_name": "performance-test-model",
            "display_name": "Performance Test Model",
            "quality_score": 0.8,
            "speed_score": 0.9,
            "cost_efficiency": 0.85,
            "reliability_score": 0.9,
            "is_active": True,
            "created_at": datetime.now()
        }
        
        result = await self.db.ai_models_test.insert_one(doc)
        return 1 if result.inserted_id else 0
        
    async def _document_update(self) -> int:
        """ë¬¸ì„œ ì—…ë°ì´íŠ¸"""
        model_id = f"test-model-{random.randint(0, 999):05d}"
        result = await self.db.ai_models_test.update_one(
            {"model_id": model_id},
            {"$set": {"updated_at": datetime.now(), "quality_score": random.uniform(0.5, 1.0)}}
        )
        return result.modified_count
        
    async def _document_delete(self) -> int:
        """ë¬¸ì„œ ì‚­ì œ (í…ŒìŠ¤íŠ¸ìš© ë¬¸ì„œë§Œ)"""
        result = await self.db.ai_models_test.delete_one({"provider": "test"})
        return result.deleted_count
        
    async def test_concurrent_operations(self, concurrent_tasks: int = 10) -> List[DBPerformanceResult]:
        """ë™ì‹œ ì—°ì‚° ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        self.logger.info(f"ë™ì‹œ ì—°ì‚° í…ŒìŠ¤íŠ¸: {concurrent_tasks}ê°œ íƒœìŠ¤í¬")
        
        async def concurrent_read_task():
            """ë™ì‹œ ì½ê¸° íƒœìŠ¤í¬"""
            times = []
            for _ in range(10):
                start_time = time.time()
                await self._multiple_find()
                end_time = time.time()
                times.append((end_time - start_time) * 1000)
            return times
            
        async def concurrent_write_task():
            """ë™ì‹œ ì“°ê¸° íƒœìŠ¤í¬"""
            times = []
            for _ in range(5):
                start_time = time.time()
                await self._document_insert()
                end_time = time.time()
                times.append((end_time - start_time) * 1000)
            return times
            
        # ì½ê¸° íƒœìŠ¤í¬ ì‹¤í–‰
        read_tasks = [concurrent_read_task() for _ in range(concurrent_tasks // 2)]
        write_tasks = [concurrent_write_task() for _ in range(concurrent_tasks // 2)]
        
        all_tasks = read_tasks + write_tasks
        results = await asyncio.gather(*all_tasks)
        
        # ê²°ê³¼ ë¶„ì„
        read_times = []
        write_times = []
        
        for i, result in enumerate(results):
            if i < len(read_tasks):
                read_times.extend(result)
            else:
                write_times.extend(result)
                
        concurrent_results = []
        
        if read_times:
            concurrent_results.append(DBPerformanceResult(
                operation="concurrent_reads",
                execution_times=read_times,
                documents_processed=len(read_times),
                errors=[],
                avg_time=statistics.mean(read_times),
                max_time=max(read_times),
                min_time=min(read_times)
            ))
            
        if write_times:
            concurrent_results.append(DBPerformanceResult(
                operation="concurrent_writes",
                execution_times=write_times,
                documents_processed=len(write_times),
                errors=[],
                avg_time=statistics.mean(write_times),
                max_time=max(write_times),
                min_time=min(write_times)
            ))
            
        return concurrent_results
        
    async def test_query_optimization(self) -> List[DBPerformanceResult]:
        """ì¿¼ë¦¬ ìµœì í™” í…ŒìŠ¤íŠ¸"""
        results = []
        
        # 1. ì¸ë±ìŠ¤ íš¨ê³¼ ë¹„êµ
        self.logger.info("ì¸ë±ìŠ¤ íš¨ê³¼ ë¹„êµ í…ŒìŠ¤íŠ¸")
        
        # ì¸ë±ìŠ¤ ì‚¬ìš© ì¿¼ë¦¬
        start_time = time.time()
        for _ in range(100):
            await self.db.ai_models_test.find_one({"model_id": f"test-model-{random.randint(0, 999):05d}"})
        indexed_time = time.time() - start_time
        
        # ì¸ë±ìŠ¤ ë¯¸ì‚¬ìš© ì¿¼ë¦¬  
        start_time = time.time()
        for _ in range(100):
            await self.db.ai_models_test.find_one({"display_name": f"Test Model {random.randint(0, 999)}"})
        unindexed_time = time.time() - start_time
        
        results.append(DBPerformanceResult(
            operation="indexed_query_comparison",
            execution_times=[indexed_time * 1000, unindexed_time * 1000],
            documents_processed=200,
            errors=[],
            avg_time=(indexed_time + unindexed_time) * 500,
            max_time=max(indexed_time, unindexed_time) * 1000,
            min_time=min(indexed_time, unindexed_time) * 1000
        ))
        
        # 2. í”„ë¡œì ì…˜ íš¨ê³¼ í…ŒìŠ¤íŠ¸
        self.logger.info("í”„ë¡œì ì…˜ íš¨ê³¼ í…ŒìŠ¤íŠ¸")
        
        # ì „ì²´ í•„ë“œ ì¡°íšŒ
        start_time = time.time()
        cursor = self.db.ai_models_test.find({"provider": "openai"}).limit(100)
        await cursor.to_list(length=100)
        full_projection_time = time.time() - start_time
        
        # ì¼ë¶€ í•„ë“œë§Œ ì¡°íšŒ
        start_time = time.time()
        cursor = self.db.ai_models_test.find(
            {"provider": "openai"}, 
            {"model_id": 1, "display_name": 1, "quality_score": 1}
        ).limit(100)
        await cursor.to_list(length=100)
        partial_projection_time = time.time() - start_time
        
        results.append(DBPerformanceResult(
            operation="projection_comparison",
            execution_times=[full_projection_time * 1000, partial_projection_time * 1000],
            documents_processed=200,
            errors=[],
            avg_time=(full_projection_time + partial_projection_time) * 500,
            max_time=max(full_projection_time, partial_projection_time) * 1000,
            min_time=min(full_projection_time, partial_projection_time) * 1000
        ))
        
        return results
        
    def analyze_results(self, results: List[DBPerformanceResult]) -> Dict:
        """ê²°ê³¼ ë¶„ì„"""
        analysis = {
            'summary': {
                'total_operations': len(results),
                'total_documents_processed': sum(r.documents_processed for r in results),
                'total_errors': sum(len(r.errors) for r in results),
                'avg_response_time': 0,
                'slowest_operation': '',
                'fastest_operation': ''
            },
            'operations': {},
            'recommendations': []
        }
        
        if not results:
            return analysis
            
        # ì—°ì‚°ë³„ ë¶„ì„
        all_avg_times = []
        for result in results:
            analysis['operations'][result.operation] = {
                'avg_time': result.avg_time,
                'max_time': result.max_time,
                'min_time': result.min_time,
                'documents_processed': result.documents_processed,
                'error_count': len(result.errors),
                'throughput': result.documents_processed / (result.avg_time / 1000) if result.avg_time > 0 else 0
            }
            all_avg_times.append(result.avg_time)
            
        # ì „ì²´ í‰ê· 
        if all_avg_times:
            analysis['summary']['avg_response_time'] = statistics.mean(all_avg_times)
            
            # ê°€ì¥ ëŠë¦°/ë¹ ë¥¸ ì—°ì‚°
            slowest = max(results, key=lambda x: x.avg_time)
            fastest = min(results, key=lambda x: x.avg_time)
            analysis['summary']['slowest_operation'] = f"{slowest.operation} ({slowest.avg_time:.2f}ms)"
            analysis['summary']['fastest_operation'] = f"{fastest.operation} ({fastest.avg_time:.2f}ms)"
            
        # ì„±ëŠ¥ ê°œì„  ê¶Œì¥ì‚¬í•­
        recommendations = []
        
        for result in results:
            if result.avg_time > 100:  # 100ms ì´ˆê³¼
                recommendations.append(f"{result.operation}: í‰ê·  ì‘ë‹µì‹œê°„ì´ {result.avg_time:.2f}msë¡œ ëŠë¦¼ - ì¸ë±ìŠ¤ ìµœì í™” í•„ìš”")
                
            if len(result.errors) > 0:
                recommendations.append(f"{result.operation}: {len(result.errors)}ê°œ ì—ëŸ¬ ë°œìƒ - ì—ëŸ¬ ì›ì¸ ë¶„ì„ í•„ìš”")
                
            if 'unindexed' in result.operation and result.avg_time > 50:
                recommendations.append(f"{result.operation}: ì¸ë±ìŠ¤ ë¯¸ì‚¬ìš© ì¿¼ë¦¬ ì„±ëŠ¥ ì €í•˜ - ì ì ˆí•œ ì¸ë±ìŠ¤ ìƒì„± ê¶Œì¥")
                
        analysis['recommendations'] = recommendations
        
        return analysis
        
    def generate_report(self, analysis: Dict) -> str:
        """DB ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = []
        report.append("=" * 60)
        report.append("AI ëª¨ë¸ ê´€ë¦¬ ì‹œìŠ¤í…œ - ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸")
        report.append("=" * 60)
        report.append(f"í…ŒìŠ¤íŠ¸ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")
        
        # ì „ì²´ ìš”ì•½
        summary = analysis['summary']
        report.append("ğŸ“Š ì „ì²´ ìš”ì•½")
        report.append("-" * 30)
        report.append(f"ì´ ì—°ì‚° ìˆ˜: {summary['total_operations']}")
        report.append(f"ì²˜ë¦¬ëœ ë¬¸ì„œ ìˆ˜: {summary['total_documents_processed']:,}")
        report.append(f"ì´ ì—ëŸ¬ ìˆ˜: {summary['total_errors']}")
        report.append(f"í‰ê·  ì‘ë‹µì‹œê°„: {summary['avg_response_time']:.2f}ms")
        report.append(f"ê°€ì¥ ëŠë¦° ì—°ì‚°: {summary['slowest_operation']}")
        report.append(f"ê°€ì¥ ë¹ ë¥¸ ì—°ì‚°: {summary['fastest_operation']}")
        report.append("")
        
        # ì—°ì‚°ë³„ ìƒì„¸ ë¶„ì„
        report.append("ğŸ” ì—°ì‚°ë³„ ì„±ëŠ¥ ë¶„ì„")
        report.append("-" * 30)
        
        for operation, data in analysis['operations'].items():
            report.append(f"ğŸ“‹ {operation}")
            report.append(f"  í‰ê·  ì‹œê°„: {data['avg_time']:.2f}ms")
            report.append(f"  ìµœëŒ€ ì‹œê°„: {data['max_time']:.2f}ms")
            report.append(f"  ìµœì†Œ ì‹œê°„: {data['min_time']:.2f}ms")
            report.append(f"  ì²˜ë¦¬ ë¬¸ì„œ: {data['documents_processed']:,}ê°œ")
            report.append(f"  ì²˜ë¦¬ëŸ‰: {data['throughput']:.2f} docs/sec")
            if data['error_count'] > 0:
                report.append(f"  ì—ëŸ¬ ìˆ˜: {data['error_count']}")
            report.append("")
            
        # ì„±ëŠ¥ ê¸°ì¤€ í‰ê°€
        report.append("ğŸ¯ ì„±ëŠ¥ í‰ê°€")
        report.append("-" * 30)
        
        performance_criteria = {
            'single_find_indexed': 5,     # 5ms ì´í•˜
            'multiple_find': 50,          # 50ms ì´í•˜
            'aggregation_pipeline': 200,  # 200ms ì´í•˜
            'document_insert': 10,        # 10ms ì´í•˜
            'document_update': 20,        # 20ms ì´í•˜
        }
        
        for operation, data in analysis['operations'].items():
            if operation in performance_criteria:
                target = performance_criteria[operation]
                status = "âœ… ìš°ìˆ˜" if data['avg_time'] <= target else "âš ï¸ ê°œì„ í•„ìš”" if data['avg_time'] <= target * 2 else "âŒ ëŠë¦¼"
                report.append(f"  {operation}: {data['avg_time']:.2f}ms (ê¸°ì¤€: â‰¤{target}ms) {status}")
                
        report.append("")
        
        # ê°œì„  ê¶Œì¥ì‚¬í•­
        if analysis['recommendations']:
            report.append("ğŸ’¡ ì„±ëŠ¥ ê°œì„  ê¶Œì¥ì‚¬í•­")
            report.append("-" * 30)
            for i, recommendation in enumerate(analysis['recommendations'], 1):
                report.append(f"  {i}. {recommendation}")
            report.append("")
            
        return "\n".join(report)

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    db_test = DatabasePerformanceTest()
    
    try:
        # ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
        await db_test.setup_database()
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        await db_test.create_test_data(1000)
        
        # ê¸°ë³¸ ì—°ì‚° í…ŒìŠ¤íŠ¸
        print("ğŸ§ª ê¸°ë³¸ ì—°ì‚° ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
        basic_results = await db_test.test_basic_operations()
        
        # ë™ì‹œ ì—°ì‚° í…ŒìŠ¤íŠ¸
        print("ğŸ”„ ë™ì‹œ ì—°ì‚° ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
        concurrent_results = await db_test.test_concurrent_operations(10)
        
        # ì¿¼ë¦¬ ìµœì í™” í…ŒìŠ¤íŠ¸
        print("âš¡ ì¿¼ë¦¬ ìµœì í™” í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
        optimization_results = await db_test.test_query_optimization()
        
        # ì „ì²´ ê²°ê³¼ ë¶„ì„
        all_results = basic_results + concurrent_results + optimization_results
        analysis = db_test.analyze_results(all_results)
        
        # ë¦¬í¬íŠ¸ ìƒì„±
        report = db_test.generate_report(analysis)
        print(report)
        
        # ê²°ê³¼ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # í…ìŠ¤íŠ¸ ë¦¬í¬íŠ¸
        report_file = f"db_performance_report_{timestamp}.txt"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
            
        # JSON ê²°ê³¼
        json_file = f"db_performance_results_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            # ê²°ê³¼ë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
            json_data = {
                'analysis': analysis,
                'raw_results': [
                    {
                        'operation': r.operation,
                        'avg_time': r.avg_time,
                        'max_time': r.max_time,
                        'min_time': r.min_time,
                        'documents_processed': r.documents_processed,
                        'error_count': len(r.errors),
                        'errors': r.errors
                    }
                    for r in all_results
                ]
            }
            json.dump(json_data, f, indent=2, ensure_ascii=False)
            
        print(f"\nğŸ“„ ë¦¬í¬íŠ¸ ì €ì¥: {report_file}")
        print(f"ğŸ“Š JSON ê²°ê³¼ ì €ì¥: {json_file}")
        
    finally:
        await db_test.cleanup_database()

if __name__ == "__main__":
    asyncio.run(main())