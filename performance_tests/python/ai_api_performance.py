#!/usr/bin/env python3
"""
AI ëª¨ë¸ ê´€ë¦¬ ì‹œìŠ¤í…œ - AI API ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
ì™¸ë¶€ AI ì„œë¹„ìŠ¤ ì„±ëŠ¥ ë° ì•ˆì •ì„± í…ŒìŠ¤íŠ¸
"""

import asyncio
import aiohttp
import time
import statistics
from typing import Dict, List, Optional
import logging
from datetime import datetime
from dataclasses import dataclass, asdict
import json
import os
from dotenv import load_dotenv
import random

load_dotenv('.env.performance')

@dataclass
class AIAPITestResult:
    """AI API í…ŒìŠ¤íŠ¸ ê²°ê³¼"""
    provider: str
    model: str
    test_type: str
    response_times: List[float]
    token_counts: List[int]
    costs: List[float]
    success_count: int
    error_count: int
    errors: List[str]
    avg_response_time: float
    avg_tokens: float
    avg_cost: float
    success_rate: float

class AIAPIPerformanceTest:
    """AI API ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.api_keys = {
            'openai': os.getenv('OPENAI_TEST_API_KEY'),
            'anthropic': os.getenv('ANTHROPIC_TEST_API_KEY'),
            'google': os.getenv('GOOGLE_TEST_API_KEY'),
            'novita': os.getenv('NOVITA_TEST_API_KEY')
        }
        
        self.session = None
        self.results: List[AIAPITestResult] = []
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        # í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë“¤
        self.test_prompts = {
            'simple': "ì•ˆë…•í•˜ì„¸ìš”. ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì–´ë–¤ê°€ìš”?",
            'evaluation': """ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ í’ˆì§ˆì„ 1-10ì ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”:
            "ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì˜ ë°œì „ìœ¼ë¡œ ìš°ë¦¬ ì‚¶ì´ ë§ì´ í¸í•´ì¡ŒìŠµë‹ˆë‹¤. íŠ¹íˆ ìë™ë²ˆì—­ê³¼ ìŒì„±ì¸ì‹ ê¸°ìˆ ì´ ë°œë‹¬í•˜ë©´ì„œ ì–¸ì–´ ì¥ë²½ì´ ë§ì´ í•´ì†Œë˜ì—ˆìŠµë‹ˆë‹¤."
            
            í‰ê°€ ê¸°ì¤€:
            1. ë¬¸ë²• ì •í™•ì„±
            2. ë‚´ìš©ì˜ ë…¼ë¦¬ì„±
            3. í‘œí˜„ì˜ ëª…í™•ì„±
            
            ì ìˆ˜ì™€ í•¨ê»˜ ê°„ë‹¨í•œ ì´ìœ ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”.""",
            'korean_analysis': """ë‹¤ìŒ í•œêµ­ì–´ ë¬¸ì¥ì„ ë¶„ì„í•´ì£¼ì„¸ìš”:
            "ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ ê¸°ìˆ ì´ ë°œì „í•˜ë©´ì„œ AIê°€ ì¸ê°„ì˜ ì°½ì‘ í™œë™ì—ë„ ì˜í–¥ì„ ë¯¸ì¹˜ê³  ìˆë‹¤."
            
            ë¶„ì„ ìš”ì†Œ:
            - ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
            - ë¬¸ì¥ êµ¬ì¡° ë¶„ì„
            - ì˜ë¯¸ í•´ì„""",
            'long_context': """ê¸´ í…ìŠ¤íŠ¸ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤. """ + "ì´ê²ƒì€ ê¸´ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ëŠ¥ë ¥ì„ í…ŒìŠ¤íŠ¸í•˜ê¸° ìœ„í•œ ë¬¸ì¥ì…ë‹ˆë‹¤. " * 50 + """
            ìœ„ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”."""
        }
        
    async def setup_session(self):
        """HTTP ì„¸ì…˜ ì„¤ì •"""
        connector = aiohttp.TCPConnector(
            limit=50,
            limit_per_host=10,
            keepalive_timeout=30
        )
        
        timeout = aiohttp.ClientTimeout(total=60, connect=10)
        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout
        )
        
    async def cleanup_session(self):
        """ì„¸ì…˜ ì •ë¦¬"""
        if self.session:
            await self.session.close()
            
    async def test_openai_api(self, iterations: int = 20) -> List[AIAPITestResult]:
        """OpenAI API ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        if not self.api_keys['openai']:
            self.logger.warning("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
            return []
            
        models = ['gpt-3.5-turbo', 'gpt-4']
        results = []
        
        for model in models:
            for test_type, prompt in self.test_prompts.items():
                result = await self._test_openai_model(model, test_type, prompt, iterations)
                results.append(result)
                
        return results
        
    async def _test_openai_model(self, model: str, test_type: str, prompt: str, iterations: int) -> AIAPITestResult:
        """ê°œë³„ OpenAI ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
        self.logger.info(f"OpenAI {model} í…ŒìŠ¤íŠ¸ ì¤‘: {test_type}")
        
        response_times = []
        token_counts = []
        costs = []
        errors = []
        success_count = 0
        
        headers = {
            'Authorization': f'Bearer {self.api_keys["openai"]}',
            'Content-Type': 'application/json'
        }
        
        for i in range(iterations):
            try:
                start_time = time.time()
                
                data = {
                    'model': model,
                    'messages': [
                        {'role': 'user', 'content': prompt}
                    ],
                    'max_tokens': 500,
                    'temperature': 0.7
                }
                
                async with self.session.post(
                    'https://api.openai.com/v1/chat/completions',
                    headers=headers,
                    json=data
                ) as response:
                    end_time = time.time()
                    response_time = (end_time - start_time) * 1000  # ms
                    
                    if response.status == 200:
                        result = await response.json()
                        
                        # í† í° ìˆ˜ ë° ë¹„ìš© ê³„ì‚°
                        usage = result.get('usage', {})
                        total_tokens = usage.get('total_tokens', 0)
                        
                        # ëª¨ë¸ë³„ ë¹„ìš© ê³„ì‚° (ì˜ˆì‹œ ê°€ê²©)
                        cost_per_1k_tokens = 0.002 if model == 'gpt-3.5-turbo' else 0.03
                        cost = (total_tokens / 1000) * cost_per_1k_tokens
                        
                        response_times.append(response_time)
                        token_counts.append(total_tokens)
                        costs.append(cost)
                        success_count += 1
                        
                    else:
                        error_text = await response.text()
                        errors.append(f"HTTP {response.status}: {error_text}")
                        
            except asyncio.TimeoutError:
                errors.append("Request timeout")
            except Exception as e:
                errors.append(f"Exception: {str(e)}")
                
            # API ìœ¨ì œí•œ ê³ ë ¤ (ìš”ì²­ ê°„ ê°„ê²©)
            await asyncio.sleep(0.1)
            
        # í†µê³„ ê³„ì‚°
        avg_response_time = statistics.mean(response_times) if response_times else 0
        avg_tokens = statistics.mean(token_counts) if token_counts else 0
        avg_cost = statistics.mean(costs) if costs else 0
        success_rate = (success_count / iterations) * 100
        
        return AIAPITestResult(
            provider='openai',
            model=model,
            test_type=test_type,
            response_times=response_times,
            token_counts=token_counts,
            costs=costs,
            success_count=success_count,
            error_count=len(errors),
            errors=errors,
            avg_response_time=avg_response_time,
            avg_tokens=avg_tokens,
            avg_cost=avg_cost,
            success_rate=success_rate
        )
        
    async def test_anthropic_api(self, iterations: int = 20) -> List[AIAPITestResult]:
        """Anthropic API ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        if not self.api_keys['anthropic']:
            self.logger.warning("Anthropic API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
            return []
            
        models = ['claude-3-haiku-20240307', 'claude-3-sonnet-20240229']
        results = []
        
        for model in models:
            for test_type, prompt in self.test_prompts.items():
                result = await self._test_anthropic_model(model, test_type, prompt, iterations)
                results.append(result)
                
        return results
        
    async def _test_anthropic_model(self, model: str, test_type: str, prompt: str, iterations: int) -> AIAPITestResult:
        """ê°œë³„ Anthropic ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
        self.logger.info(f"Anthropic {model} í…ŒìŠ¤íŠ¸ ì¤‘: {test_type}")
        
        response_times = []
        token_counts = []
        costs = []
        errors = []
        success_count = 0
        
        headers = {
            'x-api-key': self.api_keys['anthropic'],
            'Content-Type': 'application/json',
            'anthropic-version': '2023-06-01'
        }
        
        for i in range(iterations):
            try:
                start_time = time.time()
                
                data = {
                    'model': model,
                    'max_tokens': 500,
                    'messages': [
                        {'role': 'user', 'content': prompt}
                    ]
                }
                
                async with self.session.post(
                    'https://api.anthropic.com/v1/messages',
                    headers=headers,
                    json=data
                ) as response:
                    end_time = time.time()
                    response_time = (end_time - start_time) * 1000
                    
                    if response.status == 200:
                        result = await response.json()
                        
                        # í† í° ìˆ˜ ì¶”ì • (Anthropicì€ ìƒì„¸ ì‚¬ìš©ëŸ‰ ì •ë³´ ì œí•œì )
                        content = result.get('content', [])
                        if content and isinstance(content, list):
                            text = content[0].get('text', '')
                            estimated_tokens = len(text.split()) * 1.3  # ëŒ€ëµì  ì¶”ì •
                        else:
                            estimated_tokens = 100  # ê¸°ë³¸ê°’
                            
                        # ëª¨ë¸ë³„ ë¹„ìš© ê³„ì‚°
                        cost_per_1k_tokens = 0.00025 if 'haiku' in model else 0.003
                        cost = (estimated_tokens / 1000) * cost_per_1k_tokens
                        
                        response_times.append(response_time)
                        token_counts.append(int(estimated_tokens))
                        costs.append(cost)
                        success_count += 1
                        
                    else:
                        error_text = await response.text()
                        errors.append(f"HTTP {response.status}: {error_text}")
                        
            except asyncio.TimeoutError:
                errors.append("Request timeout")
            except Exception as e:
                errors.append(f"Exception: {str(e)}")
                
            await asyncio.sleep(0.1)
            
        # í†µê³„ ê³„ì‚°
        avg_response_time = statistics.mean(response_times) if response_times else 0
        avg_tokens = statistics.mean(token_counts) if token_counts else 0
        avg_cost = statistics.mean(costs) if costs else 0
        success_rate = (success_count / iterations) * 100
        
        return AIAPITestResult(
            provider='anthropic',
            model=model,
            test_type=test_type,
            response_times=response_times,
            token_counts=token_counts,
            costs=costs,
            success_count=success_count,
            error_count=len(errors),
            errors=errors,
            avg_response_time=avg_response_time,
            avg_tokens=avg_tokens,
            avg_cost=avg_cost,
            success_rate=success_rate
        )
        
    async def test_stress_scenarios(self, concurrent_requests: int = 10) -> List[AIAPITestResult]:
        """ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤"""
        self.logger.info(f"ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸: {concurrent_requests}ê°œ ë™ì‹œ ìš”ì²­")
        
        # OpenAI API ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ (API í‚¤ê°€ ìˆëŠ” ê²½ìš°)
        if self.api_keys['openai']:
            stress_result = await self._concurrent_api_test(
                'openai', 'gpt-3.5-turbo', concurrent_requests
            )
            return [stress_result]
        else:
            self.logger.warning("ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ API í‚¤ê°€ ì—†ìŒ")
            return []
            
    async def _concurrent_api_test(self, provider: str, model: str, concurrent_requests: int) -> AIAPITestResult:
        """ë™ì‹œ API ìš”ì²­ í…ŒìŠ¤íŠ¸"""
        async def single_request():
            """ë‹¨ì¼ API ìš”ì²­"""
            start_time = time.time()
            
            try:
                if provider == 'openai':
                    headers = {
                        'Authorization': f'Bearer {self.api_keys["openai"]}',
                        'Content-Type': 'application/json'
                    }
                    
                    data = {
                        'model': model,
                        'messages': [
                            {'role': 'user', 'content': self.test_prompts['simple']}
                        ],
                        'max_tokens': 100
                    }
                    
                    async with self.session.post(
                        'https://api.openai.com/v1/chat/completions',
                        headers=headers,
                        json=data
                    ) as response:
                        end_time = time.time()
                        response_time = (end_time - start_time) * 1000
                        
                        if response.status == 200:
                            result = await response.json()
                            usage = result.get('usage', {})
                            tokens = usage.get('total_tokens', 0)
                            return {
                                'success': True,
                                'response_time': response_time,
                                'tokens': tokens,
                                'error': None
                            }
                        else:
                            error_text = await response.text()
                            return {
                                'success': False,
                                'response_time': response_time,
                                'tokens': 0,
                                'error': f"HTTP {response.status}: {error_text}"
                            }
                            
            except Exception as e:
                end_time = time.time()
                return {
                    'success': False,
                    'response_time': (end_time - start_time) * 1000,
                    'tokens': 0,
                    'error': str(e)
                }
                
        # ë™ì‹œ ìš”ì²­ ì‹¤í–‰
        tasks = [single_request() for _ in range(concurrent_requests)]
        results = await asyncio.gather(*tasks)
        
        # ê²°ê³¼ ë¶„ì„
        response_times = []
        token_counts = []
        costs = []
        errors = []
        success_count = 0
        
        for result in results:
            response_times.append(result['response_time'])
            token_counts.append(result['tokens'])
            
            if result['success']:
                success_count += 1
                # ë¹„ìš© ê³„ì‚°
                cost = (result['tokens'] / 1000) * 0.002  # gpt-3.5-turbo ê¸°ì¤€
                costs.append(cost)
            else:
                errors.append(result['error'])
                costs.append(0)
                
        avg_response_time = statistics.mean(response_times) if response_times else 0
        avg_tokens = statistics.mean(token_counts) if token_counts else 0
        avg_cost = statistics.mean(costs) if costs else 0
        success_rate = (success_count / concurrent_requests) * 100
        
        return AIAPITestResult(
            provider=provider,
            model=model,
            test_type='concurrent_stress',
            response_times=response_times,
            token_counts=token_counts,
            costs=costs,
            success_count=success_count,
            error_count=len(errors),
            errors=errors,
            avg_response_time=avg_response_time,
            avg_tokens=avg_tokens,
            avg_cost=avg_cost,
            success_rate=success_rate
        )
        
    async def test_reliability_scenarios(self) -> List[AIAPITestResult]:
        """ì‹ ë¢°ì„± í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤"""
        reliability_results = []
        
        # 1. ì—°ì† ìš”ì²­ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸
        if self.api_keys['openai']:
            self.logger.info("ì—°ì† ìš”ì²­ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸")
            result = await self._continuous_requests_test('openai', 'gpt-3.5-turbo', 50)
            reliability_results.append(result)
            
        # 2. íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
        if self.api_keys['openai']:
            self.logger.info("íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸")
            result = await self._timeout_handling_test('openai', 'gpt-3.5-turbo')
            reliability_results.append(result)
            
        return reliability_results
        
    async def _continuous_requests_test(self, provider: str, model: str, request_count: int) -> AIAPITestResult:
        """ì—°ì† ìš”ì²­ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸"""
        response_times = []
        token_counts = []
        costs = []
        errors = []
        success_count = 0
        
        headers = {
            'Authorization': f'Bearer {self.api_keys[provider]}',
            'Content-Type': 'application/json'
        }
        
        for i in range(request_count):
            try:
                start_time = time.time()
                
                data = {
                    'model': model,
                    'messages': [
                        {'role': 'user', 'content': f"í…ŒìŠ¤íŠ¸ ìš”ì²­ #{i+1}: {self.test_prompts['simple']}"}
                    ],
                    'max_tokens': 100
                }
                
                async with self.session.post(
                    'https://api.openai.com/v1/chat/completions',
                    headers=headers,
                    json=data
                ) as response:
                    end_time = time.time()
                    response_time = (end_time - start_time) * 1000
                    
                    if response.status == 200:
                        result = await response.json()
                        usage = result.get('usage', {})
                        tokens = usage.get('total_tokens', 0)
                        cost = (tokens / 1000) * 0.002
                        
                        response_times.append(response_time)
                        token_counts.append(tokens)
                        costs.append(cost)
                        success_count += 1
                    else:
                        error_text = await response.text()
                        errors.append(f"Request {i+1}: HTTP {response.status}: {error_text}")
                        
            except Exception as e:
                errors.append(f"Request {i+1}: {str(e)}")
                
            # ì•ˆì •ì„± ì¸¡ì •ì„ ìœ„í•œ ê°„ê²©
            await asyncio.sleep(0.5)
            
        avg_response_time = statistics.mean(response_times) if response_times else 0
        avg_tokens = statistics.mean(token_counts) if token_counts else 0
        avg_cost = statistics.mean(costs) if costs else 0
        success_rate = (success_count / request_count) * 100
        
        return AIAPITestResult(
            provider=provider,
            model=model,
            test_type='continuous_reliability',
            response_times=response_times,
            token_counts=token_counts,
            costs=costs,
            success_count=success_count,
            error_count=len(errors),
            errors=errors,
            avg_response_time=avg_response_time,
            avg_tokens=avg_tokens,
            avg_cost=avg_cost,
            success_rate=success_rate
        )
        
    async def _timeout_handling_test(self, provider: str, model: str) -> AIAPITestResult:
        """íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        # ë§¤ìš° ê¸´ í”„ë¡¬í”„íŠ¸ë¡œ íƒ€ì„ì•„ì›ƒ ìœ ë°œ ì‹œë„
        long_prompt = "ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ë§¤ìš° ìƒì„¸íˆ ë¶„ì„í•´ì£¼ì„¸ìš”. " + "ì´ëŠ” ê¸´ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤. " * 200
        
        response_times = []
        errors = []
        success_count = 0
        
        headers = {
            'Authorization': f'Bearer {self.api_keys[provider]}',
            'Content-Type': 'application/json'
        }
        
        # ì§§ì€ íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ì„¤ì •
        short_timeout = aiohttp.ClientTimeout(total=5, connect=2)
        
        for i in range(10):
            try:
                start_time = time.time()
                
                data = {
                    'model': model,
                    'messages': [
                        {'role': 'user', 'content': long_prompt}
                    ],
                    'max_tokens': 1000
                }
                
                async with self.session.post(
                    'https://api.openai.com/v1/chat/completions',
                    headers=headers,
                    json=data,
                    timeout=short_timeout
                ) as response:
                    end_time = time.time()
                    response_time = (end_time - start_time) * 1000
                    
                    if response.status == 200:
                        response_times.append(response_time)
                        success_count += 1
                    else:
                        error_text = await response.text()
                        errors.append(f"HTTP {response.status}: {error_text}")
                        
            except asyncio.TimeoutError:
                end_time = time.time()
                response_times.append((end_time - start_time) * 1000)
                errors.append("Timeout as expected")
            except Exception as e:
                errors.append(f"Exception: {str(e)}")
                
            await asyncio.sleep(1)
            
        avg_response_time = statistics.mean(response_times) if response_times else 0
        success_rate = (success_count / 10) * 100
        
        return AIAPITestResult(
            provider=provider,
            model=model,
            test_type='timeout_handling',
            response_times=response_times,
            token_counts=[],
            costs=[],
            success_count=success_count,
            error_count=len(errors),
            errors=errors,
            avg_response_time=avg_response_time,
            avg_tokens=0,
            avg_cost=0,
            success_rate=success_rate
        )
        
    def analyze_results(self, results: List[AIAPITestResult]) -> Dict:
        """AI API í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„"""
        if not results:
            return {}
            
        analysis = {
            'summary': {
                'total_tests': len(results),
                'total_requests': sum(r.success_count + r.error_count for r in results),
                'overall_success_rate': 0,
                'avg_response_time': 0,
                'total_cost': 0,
                'fastest_provider': '',
                'most_reliable_provider': ''
            },
            'by_provider': {},
            'by_model': {},
            'by_test_type': {},
            'recommendations': []
        }
        
        # ì œê³µì—…ì²´ë³„ ë¶„ì„
        provider_stats = {}
        for result in results:
            provider = result.provider
            if provider not in provider_stats:
                provider_stats[provider] = {
                    'total_requests': 0,
                    'successful_requests': 0,
                    'total_response_time': 0,
                    'total_cost': 0,
                    'response_times': []
                }
                
            stats = provider_stats[provider]
            stats['total_requests'] += result.success_count + result.error_count
            stats['successful_requests'] += result.success_count
            stats['total_response_time'] += sum(result.response_times)
            stats['total_cost'] += sum(result.costs)
            stats['response_times'].extend(result.response_times)
            
        # ì œê³µì—…ì²´ë³„ ìš”ì•½
        for provider, stats in provider_stats.items():
            if stats['total_requests'] > 0:
                success_rate = (stats['successful_requests'] / stats['total_requests']) * 100
                avg_response_time = statistics.mean(stats['response_times']) if stats['response_times'] else 0
                
                analysis['by_provider'][provider] = {
                    'success_rate': success_rate,
                    'avg_response_time': avg_response_time,
                    'total_cost': stats['total_cost'],
                    'total_requests': stats['total_requests']
                }
                
        # ì „ì²´ ìš”ì•½ ê³„ì‚°
        all_response_times = []
        total_successful = 0
        total_requests = 0
        total_cost = 0
        
        for result in results:
            all_response_times.extend(result.response_times)
            total_successful += result.success_count
            total_requests += result.success_count + result.error_count
            total_cost += sum(result.costs)
            
        if total_requests > 0:
            analysis['summary']['overall_success_rate'] = (total_successful / total_requests) * 100
            
        if all_response_times:
            analysis['summary']['avg_response_time'] = statistics.mean(all_response_times)
            
        analysis['summary']['total_cost'] = total_cost
        
        # ê°€ì¥ ë¹ ë¥¸ ì œê³µì—…ì²´ ì°¾ê¸°
        if analysis['by_provider']:
            fastest = min(analysis['by_provider'].items(), key=lambda x: x[1]['avg_response_time'])
            analysis['summary']['fastest_provider'] = f"{fastest[0]} ({fastest[1]['avg_response_time']:.2f}ms)"
            
            # ê°€ì¥ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì œê³µì—…ì²´
            most_reliable = max(analysis['by_provider'].items(), key=lambda x: x[1]['success_rate'])
            analysis['summary']['most_reliable_provider'] = f"{most_reliable[0]} ({most_reliable[1]['success_rate']:.1f}%)"
            
        # ê¶Œì¥ì‚¬í•­ ìƒì„±
        recommendations = []
        
        for provider, stats in analysis['by_provider'].items():
            if stats['success_rate'] < 95:
                recommendations.append(f"{provider}: ì„±ê³µë¥  {stats['success_rate']:.1f}% - ì•ˆì •ì„± ê°œì„  í•„ìš”")
                
            if stats['avg_response_time'] > 5000:  # 5ì´ˆ ì´ˆê³¼
                recommendations.append(f"{provider}: í‰ê·  ì‘ë‹µì‹œê°„ {stats['avg_response_time']:.2f}ms - ì„±ëŠ¥ ìµœì í™” í•„ìš”")
                
        analysis['recommendations'] = recommendations
        
        return analysis
        
    def generate_report(self, analysis: Dict) -> str:
        """AI API ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = []
        report.append("=" * 60)
        report.append("AI ëª¨ë¸ ê´€ë¦¬ ì‹œìŠ¤í…œ - AI API ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸")
        report.append("=" * 60)
        report.append(f"í…ŒìŠ¤íŠ¸ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")
        
        # ì „ì²´ ìš”ì•½
        summary = analysis['summary']
        report.append("ğŸ“Š ì „ì²´ ìš”ì•½")
        report.append("-" * 30)
        report.append(f"ì´ í…ŒìŠ¤íŠ¸ ìˆ˜: {summary['total_tests']}")
        report.append(f"ì´ ìš”ì²­ ìˆ˜: {summary['total_requests']:,}")
        report.append(f"ì „ì²´ ì„±ê³µë¥ : {summary['overall_success_rate']:.1f}%")
        report.append(f"í‰ê·  ì‘ë‹µì‹œê°„: {summary['avg_response_time']:.2f}ms")
        report.append(f"ì´ ë¹„ìš©: ${summary['total_cost']:.4f}")
        report.append(f"ê°€ì¥ ë¹ ë¥¸ ì œê³µì—…ì²´: {summary['fastest_provider']}")
        report.append(f"ê°€ì¥ ì•ˆì •ì ì¸ ì œê³µì—…ì²´: {summary['most_reliable_provider']}")
        report.append("")
        
        # ì œê³µì—…ì²´ë³„ ë¶„ì„
        if analysis['by_provider']:
            report.append("ğŸ¢ ì œê³µì—…ì²´ë³„ ì„±ëŠ¥ ë¶„ì„")
            report.append("-" * 30)
            
            for provider, stats in analysis['by_provider'].items():
                report.append(f"ğŸ“‹ {provider.upper()}")
                report.append(f"  ì„±ê³µë¥ : {stats['success_rate']:.1f}%")
                report.append(f"  í‰ê·  ì‘ë‹µì‹œê°„: {stats['avg_response_time']:.2f}ms")
                report.append(f"  ì´ ë¹„ìš©: ${stats['total_cost']:.4f}")
                report.append(f"  ì´ ìš”ì²­: {stats['total_requests']:,}íšŒ")
                
                # ì„±ëŠ¥ ë“±ê¸‰
                if stats['success_rate'] >= 98 and stats['avg_response_time'] <= 3000:
                    grade = "ğŸ¥‡ ìš°ìˆ˜"
                elif stats['success_rate'] >= 95 and stats['avg_response_time'] <= 5000:
                    grade = "ğŸ¥ˆ ì–‘í˜¸"
                elif stats['success_rate'] >= 90:
                    grade = "ğŸ¥‰ ë³´í†µ"
                else:
                    grade = "âŒ ê°œì„ í•„ìš”"
                    
                report.append(f"  ì„±ëŠ¥ ë“±ê¸‰: {grade}")
                report.append("")
                
        # ì„±ëŠ¥ ê¸°ì¤€ í‰ê°€
        report.append("ğŸ¯ ì„±ëŠ¥ ê¸°ì¤€ í‰ê°€")
        report.append("-" * 30)
        
        criteria = {
            'success_rate': (95, '%'),
            'avg_response_time': (3000, 'ms')
        }
        
        for metric, (target, unit) in criteria.items():
            actual = summary.get(metric.replace('_', '_'), 0)
            if metric == 'success_rate':
                status = "âœ… í†µê³¼" if actual >= target else "âŒ ë¯¸ë‹¬"
                report.append(f"  ì„±ê³µë¥ : {actual:.1f}% (ê¸°ì¤€: â‰¥{target}%) {status}")
            else:
                status = "âœ… í†µê³¼" if actual <= target else "âŒ ë¯¸ë‹¬"
                report.append(f"  ì‘ë‹µì‹œê°„: {actual:.2f}ms (ê¸°ì¤€: â‰¤{target}ms) {status}")
                
        report.append("")
        
        # ê°œì„  ê¶Œì¥ì‚¬í•­
        if analysis['recommendations']:
            report.append("ğŸ’¡ ê°œì„  ê¶Œì¥ì‚¬í•­")
            report.append("-" * 30)
            for i, recommendation in enumerate(analysis['recommendations'], 1):
                report.append(f"  {i}. {recommendation}")
            report.append("")
            
        return "\n".join(report)

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    api_test = AIAPIPerformanceTest()
    
    try:
        await api_test.setup_session()
        
        # ê¸°ë³¸ API í…ŒìŠ¤íŠ¸
        print("ğŸ§ª OpenAI API ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
        openai_results = await api_test.test_openai_api(10)
        
        print("ğŸ§ª Anthropic API ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
        anthropic_results = await api_test.test_anthropic_api(10)
        
        # ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸
        print("ğŸ”¥ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
        stress_results = await api_test.test_stress_scenarios(5)
        
        # ì‹ ë¢°ì„± í…ŒìŠ¤íŠ¸
        print("ğŸ›¡ï¸ ì‹ ë¢°ì„± í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
        reliability_results = await api_test.test_reliability_scenarios()
        
        # ì „ì²´ ê²°ê³¼ ë¶„ì„
        all_results = openai_results + anthropic_results + stress_results + reliability_results
        analysis = api_test.analyze_results(all_results)
        
        # ë¦¬í¬íŠ¸ ìƒì„± ë° ì¶œë ¥
        report = api_test.generate_report(analysis)
        print(report)
        
        # ê²°ê³¼ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # í…ìŠ¤íŠ¸ ë¦¬í¬íŠ¸
        report_file = f"ai_api_performance_report_{timestamp}.txt"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
            
        # JSON ê²°ê³¼
        json_file = f"ai_api_performance_results_{timestamp}.json"
        json_data = {
            'analysis': analysis,
            'raw_results': [asdict(result) for result in all_results]
        }
        
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, indent=2, ensure_ascii=False)
            
        print(f"\nğŸ“„ ë¦¬í¬íŠ¸ ì €ì¥: {report_file}")
        print(f"ğŸ“Š JSON ê²°ê³¼ ì €ì¥: {json_file}")
        
    finally:
        await api_test.cleanup_session()

if __name__ == "__main__":
    asyncio.run(main())