#!/usr/bin/env python3
"""
AI ëª¨ë¸ ê´€ë¦¬ ì‹œìŠ¤í…œ - ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ëŸ¬ë„ˆ
ì¢…í•©ì ì¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë° ê²°ê³¼ ë¶„ì„
"""

import asyncio
import aiohttp
import time
import json
import statistics
from typing import Dict, List, Tuple
import argparse
from datetime import datetime
import logging
from dataclasses import dataclass
import psutil
import threading
from concurrent.futures import ThreadPoolExecutor
import os
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv('.env.performance')

@dataclass
class TestConfig:
    """í…ŒìŠ¤íŠ¸ ì„¤ì • í´ë˜ìŠ¤"""
    backend_url: str
    concurrent_users: int
    test_duration: int  # ì´ˆ
    ramp_up_time: int   # ì´ˆ
    scenarios: List[str]
    
@dataclass
class TestResult:
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼ í´ë˜ìŠ¤"""
    scenario: str
    response_times: List[float]
    status_codes: List[int]
    errors: List[str]
    start_time: datetime
    end_time: datetime
    
class PerformanceTestRunner:
    """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ê¸°"""
    
    def __init__(self, config: TestConfig):
        self.config = config
        self.results: List[TestResult] = []
        self.session = None
        self.auth_tokens = {}
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(f'performance_test_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
    async def setup_session(self):
        """HTTP ì„¸ì…˜ ì„¤ì •"""
        connector = aiohttp.TCPConnector(
            limit=200,  # ìµœëŒ€ ì—°ê²° ìˆ˜
            limit_per_host=50,
            keepalive_timeout=30,
            enable_cleanup_closed=True
        )
        
        timeout = aiohttp.ClientTimeout(total=30, connect=10)
        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout,
            headers={'Content-Type': 'application/json'}
        )
        
    async def cleanup_session(self):
        """ì„¸ì…˜ ì •ë¦¬"""
        if self.session:
            await self.session.close()
            
    async def authenticate_user(self, email: str, password: str) -> str:
        """ì‚¬ìš©ì ì¸ì¦ ë° í† í° íšë“"""
        if email in self.auth_tokens:
            return self.auth_tokens[email]
            
        login_data = {
            "email": email,
            "password": password
        }
        
        try:
            async with self.session.post(
                f"{self.config.backend_url}/api/auth/login",
                json=login_data
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    token = data.get('access_token')
                    self.auth_tokens[email] = token
                    return token
                else:
                    self.logger.error(f"Authentication failed for {email}: {response.status}")
                    return None
        except Exception as e:
            self.logger.error(f"Authentication error for {email}: {str(e)}")
            return None
            
    async def execute_scenario(self, scenario_name: str, user_id: int) -> TestResult:
        """ê°œë³„ ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í–‰"""
        result = TestResult(
            scenario=scenario_name,
            response_times=[],
            status_codes=[],
            errors=[],
            start_time=datetime.now(),
            end_time=None
        )
        
        # ì‚¬ìš©ìë³„ ì¸ì¦ (ë¼ìš´ë“œ ë¡œë¹ˆ)
        users = [
            ("admin@test.com", "testpass123"),
            ("secretary@test.com", "testpass123"),
            ("admin2@test.com", "testpass123"),
            ("secretary2@test.com", "testpass123")
        ]
        email, password = users[user_id % len(users)]
        
        auth_token = await self.authenticate_user(email, password)
        if not auth_token:
            result.errors.append(f"Authentication failed for user {user_id}")
            result.end_time = datetime.now()
            return result
            
        headers = {'Authorization': f'Bearer {auth_token}'}
        
        # ì‹œë‚˜ë¦¬ì˜¤ë³„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        if scenario_name == "get_models":
            await self._test_get_models(result, headers)
        elif scenario_name == "connection_test":
            await self._test_connection(result, headers)
        elif scenario_name == "performance_metrics":
            await self._test_performance_metrics(result, headers)
        elif scenario_name == "model_crud":
            await self._test_model_crud(result, headers)
        elif scenario_name == "mixed_workload":
            await self._test_mixed_workload(result, headers)
            
        result.end_time = datetime.now()
        return result
        
    async def _test_get_models(self, result: TestResult, headers: Dict):
        """ëª¨ë¸ ëª©ë¡ ì¡°íšŒ í…ŒìŠ¤íŠ¸"""
        try:
            start_time = time.time()
            async with self.session.get(
                f"{self.config.backend_url}/api/ai-models/available",
                headers=headers
            ) as response:
                end_time = time.time()
                
                result.response_times.append((end_time - start_time) * 1000)  # ms
                result.status_codes.append(response.status)
                
                if response.status != 200:
                    error_text = await response.text()
                    result.errors.append(f"GET models failed: {response.status} - {error_text}")
                    
        except asyncio.TimeoutError:
            result.errors.append("GET models timeout")
            result.response_times.append(30000)  # íƒ€ì„ì•„ì›ƒì„ 30ì´ˆë¡œ ê¸°ë¡
        except Exception as e:
            result.errors.append(f"GET models error: {str(e)}")
            
    async def _test_connection(self, result: TestResult, headers: Dict):
        """ì—°ê²° í…ŒìŠ¤íŠ¸"""
        models_to_test = [
            "openai-gpt35-turbo",
            "openai-gpt4", 
            "anthropic-claude3-haiku",
            "google-gemini-pro"
        ]
        
        for model_id in models_to_test:
            try:
                start_time = time.time()
                test_data = {"model_id": model_id}
                
                async with self.session.post(
                    f"{self.config.backend_url}/api/ai-models/test-connection",
                    json=test_data,
                    headers=headers
                ) as response:
                    end_time = time.time()
                    
                    result.response_times.append((end_time - start_time) * 1000)
                    result.status_codes.append(response.status)
                    
                    if response.status != 200:
                        error_text = await response.text()
                        result.errors.append(f"Connection test failed for {model_id}: {response.status} - {error_text}")
                        
                # ì—°ê²° í…ŒìŠ¤íŠ¸ ê°„ ê°„ê²©
                await asyncio.sleep(1)
                
            except asyncio.TimeoutError:
                result.errors.append(f"Connection test timeout for {model_id}")
                result.response_times.append(30000)
            except Exception as e:
                result.errors.append(f"Connection test error for {model_id}: {str(e)}")
                
    async def _test_performance_metrics(self, result: TestResult, headers: Dict):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ í…ŒìŠ¤íŠ¸"""
        endpoints = [
            "/api/ai-models/performance-metrics",
            "/api/ai-models/health/status"
        ]
        
        for endpoint in endpoints:
            try:
                start_time = time.time()
                async with self.session.get(
                    f"{self.config.backend_url}{endpoint}",
                    headers=headers
                ) as response:
                    end_time = time.time()
                    
                    result.response_times.append((end_time - start_time) * 1000)
                    result.status_codes.append(response.status)
                    
                    if response.status != 200:
                        error_text = await response.text()
                        result.errors.append(f"Metrics request failed for {endpoint}: {response.status} - {error_text}")
                        
            except asyncio.TimeoutError:
                result.errors.append(f"Metrics request timeout for {endpoint}")
                result.response_times.append(30000)
            except Exception as e:
                result.errors.append(f"Metrics request error for {endpoint}: {str(e)}")
                
    async def _test_model_crud(self, result: TestResult, headers: Dict):
        """ëª¨ë¸ CRUD í…ŒìŠ¤íŠ¸ (ê´€ë¦¬ìë§Œ)"""
        if "admin" not in headers.get('Authorization', ''):
            # ê´€ë¦¬ìê°€ ì•„ë‹Œ ê²½ìš° ìŠ¤í‚µ
            return
            
        model_id = f"test-model-{int(time.time())}-{asyncio.current_task().get_name()}"
        
        # 1. ëª¨ë¸ ìƒì„±
        try:
            start_time = time.time()
            create_data = {
                "model_id": model_id,
                "provider": "openai",
                "model_name": "gpt-3.5-turbo",
                "display_name": f"Performance Test Model {model_id}",
                "quality_score": 0.8,
                "speed_score": 0.9,
                "cost_efficiency": 0.85,
                "reliability_score": 0.9
            }
            
            async with self.session.post(
                f"{self.config.backend_url}/api/ai-models/create",
                json=create_data,
                headers=headers
            ) as response:
                end_time = time.time()
                
                result.response_times.append((end_time - start_time) * 1000)
                result.status_codes.append(response.status)
                
                if response.status != 201:
                    error_text = await response.text()
                    result.errors.append(f"Model creation failed: {response.status} - {error_text}")
                    return
                    
        except Exception as e:
            result.errors.append(f"Model creation error: {str(e)}")
            return
            
        # 2. ëª¨ë¸ ì¡°íšŒ
        try:
            start_time = time.time()
            async with self.session.get(
                f"{self.config.backend_url}/api/ai-models/{model_id}",
                headers=headers
            ) as response:
                end_time = time.time()
                
                result.response_times.append((end_time - start_time) * 1000)
                result.status_codes.append(response.status)
                
        except Exception as e:
            result.errors.append(f"Model retrieval error: {str(e)}")
            
        # 3. ëª¨ë¸ ìˆ˜ì •
        try:
            start_time = time.time()
            update_data = {
                "display_name": f"Updated {model_id}",
                "quality_score": 0.85
            }
            
            async with self.session.put(
                f"{self.config.backend_url}/api/ai-models/{model_id}",
                json=update_data,
                headers=headers
            ) as response:
                end_time = time.time()
                
                result.response_times.append((end_time - start_time) * 1000)
                result.status_codes.append(response.status)
                
        except Exception as e:
            result.errors.append(f"Model update error: {str(e)}")
            
        # 4. ëª¨ë¸ ì‚­ì œ (ì •ë¦¬)
        try:
            start_time = time.time()
            async with self.session.delete(
                f"{self.config.backend_url}/api/ai-models/{model_id}",
                headers=headers
            ) as response:
                end_time = time.time()
                
                result.response_times.append((end_time - start_time) * 1000)
                result.status_codes.append(response.status)
                
        except Exception as e:
            result.errors.append(f"Model deletion error: {str(e)}")
            
    async def _test_mixed_workload(self, result: TestResult, headers: Dict):
        """í˜¼í•© ì›Œí¬ë¡œë“œ í…ŒìŠ¤íŠ¸"""
        # ì‹¤ì œ ì‚¬ìš© íŒ¨í„´ì„ ì‹œë®¬ë ˆì´ì…˜
        workload = [
            ("get_models", 0.4),      # 40% - ëª¨ë¸ ëª©ë¡ ì¡°íšŒ
            ("performance_metrics", 0.3),  # 30% - ì„±ëŠ¥ ë©”íŠ¸ë¦­
            ("connection_test", 0.2),      # 20% - ì—°ê²° í…ŒìŠ¤íŠ¸
            ("model_crud", 0.1)            # 10% - ëª¨ë¸ ê´€ë¦¬
        ]
        
        import random
        
        for _ in range(5):  # 5ë²ˆ ë°˜ë³µ
            # ê°€ì¤‘ì¹˜ì— ë”°ë¥¸ ëœë¤ ì„ íƒ
            rand = random.random()
            cumulative = 0
            selected_test = "get_models"  # ê¸°ë³¸ê°’
            
            for test_name, weight in workload:
                cumulative += weight
                if rand <= cumulative:
                    selected_test = test_name
                    break
                    
            # ì„ íƒëœ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            if selected_test == "get_models":
                await self._test_get_models(result, headers)
            elif selected_test == "performance_metrics":
                await self._test_performance_metrics(result, headers)
            elif selected_test == "connection_test":
                await self._test_connection(result, headers)
            elif selected_test == "model_crud":
                await self._test_model_crud(result, headers)
                
            # ì‚¬ìš©ì ì‚¬ê³ ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜
            await asyncio.sleep(random.uniform(1, 3))
            
    async def run_load_test(self) -> List[TestResult]:
        """ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        self.logger.info(f"Starting load test with {self.config.concurrent_users} concurrent users")
        self.logger.info(f"Test duration: {self.config.test_duration} seconds")
        self.logger.info(f"Scenarios: {self.config.scenarios}")
        
        await self.setup_session()
        
        try:
            # ì‚¬ìš©ìë³„ íƒœìŠ¤í¬ ìƒì„±
            tasks = []
            start_time = time.time()
            
            for user_id in range(self.config.concurrent_users):
                for scenario in self.config.scenarios:
                    # ë¨í”„ì—… ì‹œê°„ì— ë”°ë¥¸ ì§€ì—° ì‹œì‘
                    delay = (user_id * self.config.ramp_up_time) / self.config.concurrent_users
                    
                    task = asyncio.create_task(
                        self._run_user_scenario(scenario, user_id, delay)
                    )
                    tasks.append(task)
                    
            # ëª¨ë“  íƒœìŠ¤í¬ ì™„ë£Œ ëŒ€ê¸°
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # ì˜ˆì™¸ ì²˜ë¦¬ ë° ê²°ê³¼ ìˆ˜ì§‘
            valid_results = []
            for result in results:
                if isinstance(result, Exception):
                    self.logger.error(f"Task failed with exception: {result}")
                else:
                    valid_results.append(result)
                    
            self.results = valid_results
            
            end_time = time.time()
            self.logger.info(f"Load test completed in {end_time - start_time:.2f} seconds")
            
            return self.results
            
        finally:
            await self.cleanup_session()
            
    async def _run_user_scenario(self, scenario: str, user_id: int, delay: float) -> TestResult:
        """ê°œë³„ ì‚¬ìš©ì ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í–‰"""
        # ë¨í”„ì—… ì§€ì—°
        if delay > 0:
            await asyncio.sleep(delay)
            
        return await self.execute_scenario(scenario, user_id)
        
    def analyze_results(self) -> Dict:
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„"""
        if not self.results:
            return {}
            
        analysis = {
            'summary': {
                'total_requests': 0,
                'total_errors': 0,
                'error_rate': 0,
                'avg_response_time': 0,
                'median_response_time': 0,
                'p95_response_time': 0,
                'p99_response_time': 0,
                'min_response_time': 0,
                'max_response_time': 0
            },
            'by_scenario': {},
            'status_codes': {},
            'errors': []
        }
        
        all_response_times = []
        all_status_codes = []
        all_errors = []
        
        for result in self.results:
            scenario = result.scenario
            
            # ì‹œë‚˜ë¦¬ì˜¤ë³„ ë¶„ì„
            if scenario not in analysis['by_scenario']:
                analysis['by_scenario'][scenario] = {
                    'requests': 0,
                    'errors': 0,
                    'avg_response_time': 0,
                    'response_times': []
                }
                
            scenario_data = analysis['by_scenario'][scenario]
            scenario_data['requests'] += len(result.response_times)
            scenario_data['errors'] += len(result.errors)
            scenario_data['response_times'].extend(result.response_times)
            
            # ì „ì²´ ë°ì´í„° ìˆ˜ì§‘
            all_response_times.extend(result.response_times)
            all_status_codes.extend(result.status_codes)
            all_errors.extend(result.errors)
            
        # ì „ì²´ ìš”ì•½ í†µê³„
        if all_response_times:
            analysis['summary']['total_requests'] = len(all_response_times)
            analysis['summary']['total_errors'] = len(all_errors)
            analysis['summary']['error_rate'] = (len(all_errors) / len(all_response_times)) * 100
            analysis['summary']['avg_response_time'] = statistics.mean(all_response_times)
            analysis['summary']['median_response_time'] = statistics.median(all_response_times)
            
            sorted_times = sorted(all_response_times)
            analysis['summary']['p95_response_time'] = sorted_times[int(0.95 * len(sorted_times))]
            analysis['summary']['p99_response_time'] = sorted_times[int(0.99 * len(sorted_times))]
            analysis['summary']['min_response_time'] = min(all_response_times)
            analysis['summary']['max_response_time'] = max(all_response_times)
            
        # ì‹œë‚˜ë¦¬ì˜¤ë³„ í‰ê·  ê³„ì‚°
        for scenario, data in analysis['by_scenario'].items():
            if data['response_times']:
                data['avg_response_time'] = statistics.mean(data['response_times'])
                
        # ìƒíƒœ ì½”ë“œ ë¶„í¬
        for code in all_status_codes:
            analysis['status_codes'][code] = analysis['status_codes'].get(code, 0) + 1
            
        # ì—ëŸ¬ ëª©ë¡
        analysis['errors'] = all_errors
        
        return analysis
        
    def generate_report(self, analysis: Dict) -> str:
        """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = []
        report.append("=" * 60)
        report.append("AI ëª¨ë¸ ê´€ë¦¬ ì‹œìŠ¤í…œ - ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸")
        report.append("=" * 60)
        report.append(f"í…ŒìŠ¤íŠ¸ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"ë™ì‹œ ì‚¬ìš©ì: {self.config.concurrent_users}")
        report.append(f"í…ŒìŠ¤íŠ¸ ì‹œê°„: {self.config.test_duration}ì´ˆ")
        report.append("")
        
        # ì „ì²´ ìš”ì•½
        summary = analysis['summary']
        report.append("ğŸ“Š ì „ì²´ ìš”ì•½")
        report.append("-" * 30)
        report.append(f"ì´ ìš”ì²­ ìˆ˜: {summary['total_requests']:,}")
        report.append(f"ì´ ì—ëŸ¬ ìˆ˜: {summary['total_errors']:,}")
        report.append(f"ì—ëŸ¬ìœ¨: {summary['error_rate']:.2f}%")
        report.append(f"í‰ê·  ì‘ë‹µì‹œê°„: {summary['avg_response_time']:.2f}ms")
        report.append(f"ì¤‘ê°„ê°’ ì‘ë‹µì‹œê°„: {summary['median_response_time']:.2f}ms")
        report.append(f"95% ì‘ë‹µì‹œê°„: {summary['p95_response_time']:.2f}ms")
        report.append(f"99% ì‘ë‹µì‹œê°„: {summary['p99_response_time']:.2f}ms")
        report.append(f"ìµœì†Œ ì‘ë‹µì‹œê°„: {summary['min_response_time']:.2f}ms")
        report.append(f"ìµœëŒ€ ì‘ë‹µì‹œê°„: {summary['max_response_time']:.2f}ms")
        report.append("")
        
        # ì‹œë‚˜ë¦¬ì˜¤ë³„ ë¶„ì„
        report.append("ğŸ­ ì‹œë‚˜ë¦¬ì˜¤ë³„ ë¶„ì„")
        report.append("-" * 30)
        for scenario, data in analysis['by_scenario'].items():
            report.append(f"ğŸ“‹ {scenario}")
            report.append(f"  ìš”ì²­ ìˆ˜: {data['requests']:,}")
            report.append(f"  ì—ëŸ¬ ìˆ˜: {data['errors']:,}")
            if data['requests'] > 0:
                error_rate = (data['errors'] / data['requests']) * 100
                report.append(f"  ì—ëŸ¬ìœ¨: {error_rate:.2f}%")
            report.append(f"  í‰ê·  ì‘ë‹µì‹œê°„: {data['avg_response_time']:.2f}ms")
            report.append("")
            
        # ìƒíƒœ ì½”ë“œ ë¶„í¬
        report.append("ğŸ“ˆ HTTP ìƒíƒœ ì½”ë“œ ë¶„í¬")
        report.append("-" * 30)
        for code, count in sorted(analysis['status_codes'].items()):
            percentage = (count / summary['total_requests']) * 100
            report.append(f"  {code}: {count:,} ({percentage:.2f}%)")
        report.append("")
        
        # ì„±ëŠ¥ í‰ê°€
        report.append("ğŸ¯ ì„±ëŠ¥ í‰ê°€")
        report.append("-" * 30)
        
        # ëª©í‘œ ëŒ€ë¹„ í‰ê°€
        goals = {
            'avg_response_time': 2000,  # 2ì´ˆ
            'p95_response_time': 5000,  # 5ì´ˆ
            'error_rate': 1.0           # 1%
        }
        
        for metric, target in goals.items():
            actual = summary[metric]
            if metric == 'error_rate':
                status = "âœ… í†µê³¼" if actual <= target else "âŒ ì‹¤íŒ¨"
                report.append(f"  {metric}: {actual:.2f}% (ëª©í‘œ: â‰¤{target}%) {status}")
            else:
                status = "âœ… í†µê³¼" if actual <= target else "âŒ ì‹¤íŒ¨"
                report.append(f"  {metric}: {actual:.2f}ms (ëª©í‘œ: â‰¤{target}ms) {status}")
                
        report.append("")
        
        # ì—ëŸ¬ ìƒì„¸ (ìµœëŒ€ 10ê°œ)
        if analysis['errors']:
            report.append("âŒ ì£¼ìš” ì—ëŸ¬ ëª©ë¡ (ìµœëŒ€ 10ê°œ)")
            report.append("-" * 30)
            for error in analysis['errors'][:10]:
                report.append(f"  â€¢ {error}")
            if len(analysis['errors']) > 10:
                report.append(f"  ... ë° {len(analysis['errors']) - 10}ê°œ ë”")
            report.append("")
            
        return "\n".join(report)

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='AI Model Management Performance Test')
    parser.add_argument('--users', type=int, default=50, help='Number of concurrent users')
    parser.add_argument('--duration', type=int, default=300, help='Test duration in seconds')
    parser.add_argument('--rampup', type=int, default=60, help='Ramp-up time in seconds')
    parser.add_argument('--scenarios', nargs='+', 
                       default=['get_models', 'connection_test', 'performance_metrics'],
                       help='Test scenarios to run')
    parser.add_argument('--backend-url', default=os.getenv('BACKEND_URL', 'http://localhost:8000'),
                       help='Backend URL')
    
    args = parser.parse_args()
    
    # í…ŒìŠ¤íŠ¸ ì„¤ì •
    config = TestConfig(
        backend_url=args.backend_url,
        concurrent_users=args.users,
        test_duration=args.duration,
        ramp_up_time=args.rampup,
        scenarios=args.scenarios
    )
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    runner = PerformanceTestRunner(config)
    
    async def run_test():
        results = await runner.run_load_test()
        analysis = runner.analyze_results()
        report = runner.generate_report(analysis)
        
        # ê²°ê³¼ ì¶œë ¥ ë° ì €ì¥
        print(report)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = f"performance_report_{timestamp}.txt"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
            
        # JSON ê²°ê³¼ë„ ì €ì¥
        json_file = f"performance_results_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, indent=2, ensure_ascii=False)
            
        print(f"\në¦¬í¬íŠ¸ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {report_file}")
        print(f"JSON ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {json_file}")
        
    # ë¹„ë™ê¸° ì‹¤í–‰
    asyncio.run(run_test())

if __name__ == "__main__":
    main()