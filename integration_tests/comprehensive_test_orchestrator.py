#!/usr/bin/env python3
"""
ğŸ”§ AI ëª¨ë¸ ê´€ë¦¬ ì‹œìŠ¤í…œ - ì¢…í•© í†µí•© í…ŒìŠ¤íŠ¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°
ëª¨ë“  í…ŒìŠ¤íŠ¸ë¥¼ í†µí•© ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë¶„ì„í•˜ëŠ” ë§ˆìŠ¤í„° ì»¨íŠ¸ë¡¤ëŸ¬
"""

import asyncio
import json
import os
import sys
import time
import subprocess
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from pathlib import Path
import logging
import yaml
import psutil
import aiohttp
from concurrent.futures import ThreadPoolExecutor, as_completed

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

@dataclass
class TestResult:
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    suite_name: str
    status: str  # success, failure, error, skipped
    execution_time: float
    total_tests: int
    passed_tests: int
    failed_tests: int
    skipped_tests: int
    error_details: List[str]
    performance_metrics: Dict[str, Any]
    coverage_data: Dict[str, float]
    
@dataclass
class SystemMetrics:
    """ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ë°ì´í„° í´ë˜ìŠ¤"""
    timestamp: datetime
    cpu_usage: float
    memory_usage: float
    memory_total: float
    disk_usage: float
    network_io: Dict[str, int]
    active_connections: int

class ComprehensiveTestOrchestrator:
    """ì¢…í•© í…ŒìŠ¤íŠ¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°"""
    
    def __init__(self, config_file: str = "integration_tests/config/test_config.yml"):
        self.config = self._load_config(config_file)
        self.results: List[TestResult] = []
        self.system_metrics: List[SystemMetrics] = []
        self.start_time = None
        self.end_time = None
        
        # ë¡œê¹… ì„¤ì •
        self._setup_logging()
        self.logger = logging.getLogger(__name__)
        
        # í…ŒìŠ¤íŠ¸ í™˜ê²½ ë³€ìˆ˜
        self.test_env = {
            'PYTHONPATH': str(PROJECT_ROOT),
            'TEST_MODE': 'integration',
            'LOG_LEVEL': 'INFO'
        }
        
    def _setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        logging.basicConfig(
            level=logging.INFO,
            format=log_format,
            handlers=[
                logging.FileHandler(f'integration_test_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
                logging.StreamHandler(sys.stdout)
            ]
        )
        
    def _load_config(self, config_file: str) -> Dict:
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except FileNotFoundError:
            self.logger.warning(f"ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {config_file}")
            return self._get_default_config()
            
    def _get_default_config(self) -> Dict:
        """ê¸°ë³¸ ì„¤ì • ë°˜í™˜"""
        return {
            'test_suites': {
                'environment_setup': {'enabled': True, 'timeout': 300},
                'functional_tests': {'enabled': True, 'timeout': 1800},
                'performance_tests': {'enabled': True, 'timeout': 900},
                'security_tests': {'enabled': True, 'timeout': 600},
                'ui_e2e_tests': {'enabled': True, 'timeout': 1200}
            },
            'system_monitoring': {
                'enabled': True,
                'interval': 10,  # ì´ˆ
                'metrics': ['cpu', 'memory', 'disk', 'network']
            },
            'reporting': {
                'formats': ['json', 'html', 'junit'],
                'include_logs': True,
                'include_screenshots': True
            }
        }
        
    async def run_comprehensive_tests(self) -> Dict[str, Any]:
        """ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        self.logger.info("ğŸš€ AI ëª¨ë¸ ê´€ë¦¬ ì‹œìŠ¤í…œ ì¢…í•© í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
        self.start_time = datetime.now()
        
        try:
            # ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ì‹œì‘
            monitoring_task = None
            if self.config.get('system_monitoring', {}).get('enabled', True):
                monitoring_task = asyncio.create_task(self._monitor_system_metrics())
                
            # 1. í™˜ê²½ ì„¤ì • ë° ì¤€ë¹„
            await self._setup_test_environment()
            
            # 2. ë³‘ë ¬ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            await self._run_parallel_test_suites()
            
            # 3. ê²°ê³¼ ìˆ˜ì§‘ ë° ë¶„ì„
            analysis_results = await self._analyze_test_results()
            
            # 4. ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
            await self._generate_comprehensive_reports(analysis_results)
            
            self.end_time = datetime.now()
            
            # ëª¨ë‹ˆí„°ë§ íƒœìŠ¤í¬ ì •ë¦¬
            if monitoring_task:
                monitoring_task.cancel()
                try:
                    await monitoring_task
                except asyncio.CancelledError:
                    pass
                    
            self.logger.info("âœ… ì¢…í•© í†µí•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
            return analysis_results
            
        except Exception as e:
            self.logger.error(f"âŒ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise
            
    async def _setup_test_environment(self):
        """í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì •"""
        self.logger.info("ğŸ“‹ í…ŒìŠ¤íŠ¸ í™˜ê²½ ì¤€ë¹„ ì¤‘...")
        
        start_time = time.time()
        
        try:
            # Docker í™˜ê²½ í™•ì¸ ë° ì‹œì‘
            await self._setup_docker_environment()
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
            await self._initialize_test_database()
            
            # í…ŒìŠ¤íŠ¸ ì‚¬ìš©ì ë° ë°ì´í„° ìƒì„±
            await self._create_test_data()
            
            # ì„œë¹„ìŠ¤ í—¬ìŠ¤ ì²´í¬
            await self._verify_services_health()
            
            execution_time = time.time() - start_time
            
            result = TestResult(
                suite_name="environment_setup",
                status="success",
                execution_time=execution_time,
                total_tests=4,
                passed_tests=4,
                failed_tests=0,
                skipped_tests=0,
                error_details=[],
                performance_metrics={"setup_time": execution_time},
                coverage_data={}
            )
            self.results.append(result)
            
            self.logger.info(f"âœ… í…ŒìŠ¤íŠ¸ í™˜ê²½ ì¤€ë¹„ ì™„ë£Œ ({execution_time:.2f}ì´ˆ)")
            
        except Exception as e:
            self.logger.error(f"âŒ í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì • ì‹¤íŒ¨: {e}")
            result = TestResult(
                suite_name="environment_setup",
                status="failure",
                execution_time=time.time() - start_time,
                total_tests=4,
                passed_tests=0,
                failed_tests=4,
                skipped_tests=0,
                error_details=[str(e)],
                performance_metrics={},
                coverage_data={}
            )
            self.results.append(result)
            raise
            
    async def _setup_docker_environment(self):
        """Docker í™˜ê²½ ì„¤ì •"""
        self.logger.info("ğŸ³ Docker í™˜ê²½ ì„¤ì • ì¤‘...")
        
        # Docker Compose ì‹¤í–‰
        result = subprocess.run(
            ['docker-compose', '-f', 'docker-compose.yml', 'up', '-d'],
            cwd=PROJECT_ROOT,
            capture_output=True,
            text=True
        )
        
        if result.returncode != 0:
            raise Exception(f"Docker í™˜ê²½ ì‹œì‘ ì‹¤íŒ¨: {result.stderr}")
            
        # ì„œë¹„ìŠ¤ ì‹œì‘ ëŒ€ê¸°
        await asyncio.sleep(30)
        
    async def _initialize_test_database(self):
        """í…ŒìŠ¤íŠ¸ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        self.logger.info("ğŸ—„ï¸ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
        
        # MongoDB ì´ˆê¸°í™” ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
        result = subprocess.run(
            ['python', 'scripts/init_test_database.py'],
            cwd=PROJECT_ROOT,
            capture_output=True,
            text=True,
            env={**os.environ, **self.test_env}
        )
        
        if result.returncode != 0:
            raise Exception(f"ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {result.stderr}")
            
    async def _create_test_data(self):
        """í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±"""
        self.logger.info("ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì¤‘...")
        
        # í…ŒìŠ¤íŠ¸ ì‚¬ìš©ì ìƒì„±
        result = subprocess.run(
            ['python', 'scripts/create_test_users.py'],
            cwd=PROJECT_ROOT,
            capture_output=True,
            text=True,
            env={**os.environ, **self.test_env}
        )
        
        if result.returncode != 0:
            raise Exception(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {result.stderr}")
            
    async def _verify_services_health(self):
        """ì„œë¹„ìŠ¤ í—¬ìŠ¤ ì²´í¬"""
        self.logger.info("ğŸ¥ ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸ ì¤‘...")
        
        services = [
            {'name': 'Frontend', 'url': 'http://localhost:3000'},
            {'name': 'Backend', 'url': 'http://localhost:8000/health'},
            {'name': 'MongoDB', 'url': 'http://localhost:8000/api/health/database'},
        ]
        
        async with aiohttp.ClientSession() as session:
            for service in services:
                try:
                    async with session.get(service['url'], timeout=10) as response:
                        if response.status == 200:
                            self.logger.info(f"âœ… {service['name']} ì„œë¹„ìŠ¤ ì •ìƒ")
                        else:
                            raise Exception(f"{service['name']} ì‘ë‹µ ì½”ë“œ: {response.status}")
                except Exception as e:
                    raise Exception(f"{service['name']} í—¬ìŠ¤ ì²´í¬ ì‹¤íŒ¨: {e}")
                    
    async def _run_parallel_test_suites(self):
        """ë³‘ë ¬ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ì‹¤í–‰"""
        self.logger.info("ğŸ”„ ë³‘ë ¬ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ì‹¤í–‰ ì¤‘...")
        
        test_suites = self.config.get('test_suites', {})
        
        # ë³‘ë ¬ ì‹¤í–‰í•  í…ŒìŠ¤íŠ¸ ì •ì˜
        parallel_tests = []
        
        if test_suites.get('functional_tests', {}).get('enabled', True):
            parallel_tests.append(self._run_functional_tests())
            
        if test_suites.get('performance_tests', {}).get('enabled', True):
            parallel_tests.append(self._run_performance_tests())
            
        if test_suites.get('security_tests', {}).get('enabled', True):
            parallel_tests.append(self._run_security_tests())
            
        if test_suites.get('ui_e2e_tests', {}).get('enabled', True):
            parallel_tests.append(self._run_ui_e2e_tests())
            
        # ëª¨ë“  í…ŒìŠ¤íŠ¸ ë³‘ë ¬ ì‹¤í–‰
        if parallel_tests:
            await asyncio.gather(*parallel_tests, return_exceptions=True)
            
    async def _run_functional_tests(self):
        """ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        self.logger.info("ğŸ”§ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
        
        start_time = time.time()
        
        try:
            # Pytest ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            result = subprocess.run(
                ['python', '-m', 'pytest', 'tests/', '-v', '--junit-xml=test-results/functional.xml', '--cov=backend', '--cov-report=json:test-results/functional-coverage.json'],
                cwd=PROJECT_ROOT,
                capture_output=True,
                text=True,
                env={**os.environ, **self.test_env},
                timeout=1800  # 30ë¶„
            )
            
            execution_time = time.time() - start_time
            
            # ê²°ê³¼ íŒŒì‹±
            passed_tests, failed_tests, skipped_tests = self._parse_pytest_output(result.stdout)
            
            # ì»¤ë²„ë¦¬ì§€ ë°ì´í„° ë¡œë“œ
            coverage_data = self._load_coverage_data('test-results/functional-coverage.json')
            
            test_result = TestResult(
                suite_name="functional_tests",
                status="success" if result.returncode == 0 else "failure",
                execution_time=execution_time,
                total_tests=passed_tests + failed_tests + skipped_tests,
                passed_tests=passed_tests,
                failed_tests=failed_tests,
                skipped_tests=skipped_tests,
                error_details=[] if result.returncode == 0 else [result.stderr],
                performance_metrics={"execution_time": execution_time},
                coverage_data=coverage_data
            )
            
            self.results.append(test_result)
            self.logger.info(f"âœ… ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {passed_tests}ê°œ ì„±ê³µ, {failed_tests}ê°œ ì‹¤íŒ¨")
            
        except subprocess.TimeoutExpired:
            self.logger.error("âŒ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ íƒ€ì„ì•„ì›ƒ")
            self.results.append(TestResult(
                suite_name="functional_tests",
                status="error",
                execution_time=time.time() - start_time,
                total_tests=0,
                passed_tests=0,
                failed_tests=0,
                skipped_tests=0,
                error_details=["Test timeout"],
                performance_metrics={},
                coverage_data={}
            ))
            
    async def _run_performance_tests(self):
        """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        self.logger.info("ğŸ“Š ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
        
        start_time = time.time()
        
        try:
            result = subprocess.run(
                ['python', 'performance_tests/python/performance_test_runner.py', '--users', '50', '--duration', '600'],
                cwd=PROJECT_ROOT,
                capture_output=True,
                text=True,
                env={**os.environ, **self.test_env},
                timeout=900  # 15ë¶„
            )
            
            execution_time = time.time() - start_time
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ íŒŒì‹±
            performance_metrics = self._parse_performance_results()
            
            test_result = TestResult(
                suite_name="performance_tests",
                status="success" if result.returncode == 0 else "failure",
                execution_time=execution_time,
                total_tests=1,
                passed_tests=1 if result.returncode == 0 else 0,
                failed_tests=0 if result.returncode == 0 else 1,
                skipped_tests=0,
                error_details=[] if result.returncode == 0 else [result.stderr],
                performance_metrics=performance_metrics,
                coverage_data={}
            )
            
            self.results.append(test_result)
            self.logger.info("âœ… ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
            
        except subprocess.TimeoutExpired:
            self.logger.error("âŒ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ íƒ€ì„ì•„ì›ƒ")
            self.results.append(TestResult(
                suite_name="performance_tests",
                status="error",
                execution_time=time.time() - start_time,
                total_tests=1,
                passed_tests=0,
                failed_tests=1,
                skipped_tests=0,
                error_details=["Performance test timeout"],
                performance_metrics={},
                coverage_data={}
            ))
            
    async def _run_security_tests(self):
        """ë³´ì•ˆ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        self.logger.info("ğŸ”’ ë³´ì•ˆ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
        
        start_time = time.time()
        
        try:
            result = subprocess.run(
                ['python', 'security_tests/run_security_tests.py', '--test-mode', 'safe'],
                cwd=PROJECT_ROOT,
                capture_output=True,
                text=True,
                env={**os.environ, **self.test_env},
                timeout=600  # 10ë¶„
            )
            
            execution_time = time.time() - start_time
            
            # ë³´ì•ˆ ê²°ê³¼ íŒŒì‹±
            security_metrics = self._parse_security_results()
            
            test_result = TestResult(
                suite_name="security_tests",
                status="success" if result.returncode == 0 else "failure",
                execution_time=execution_time,
                total_tests=1,
                passed_tests=1 if result.returncode == 0 else 0,
                failed_tests=0 if result.returncode == 0 else 1,
                skipped_tests=0,
                error_details=[] if result.returncode == 0 else [result.stderr],
                performance_metrics=security_metrics,
                coverage_data={}
            )
            
            self.results.append(test_result)
            self.logger.info("âœ… ë³´ì•ˆ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
            
        except subprocess.TimeoutExpired:
            self.logger.error("âŒ ë³´ì•ˆ í…ŒìŠ¤íŠ¸ íƒ€ì„ì•„ì›ƒ")
            self.results.append(TestResult(
                suite_name="security_tests",
                status="error",
                execution_time=time.time() - start_time,
                total_tests=1,
                passed_tests=0,
                failed_tests=1,
                skipped_tests=0,
                error_details=["Security test timeout"],
                performance_metrics={},
                coverage_data={}
            ))
            
    async def _run_ui_e2e_tests(self):
        """UI/E2E í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        self.logger.info("ğŸŒ UI/E2E í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
        
        start_time = time.time()
        
        try:
            result = subprocess.run(
                ['npx', 'playwright', 'test', '--reporter=junit'],
                cwd=PROJECT_ROOT / 'frontend',
                capture_output=True,
                text=True,
                env={**os.environ, **self.test_env},
                timeout=1200  # 20ë¶„
            )
            
            execution_time = time.time() - start_time
            
            # Playwright ê²°ê³¼ íŒŒì‹±
            passed_tests, failed_tests, skipped_tests = self._parse_playwright_output(result.stdout)
            
            test_result = TestResult(
                suite_name="ui_e2e_tests",
                status="success" if result.returncode == 0 else "failure",
                execution_time=execution_time,
                total_tests=passed_tests + failed_tests + skipped_tests,
                passed_tests=passed_tests,
                failed_tests=failed_tests,
                skipped_tests=skipped_tests,
                error_details=[] if result.returncode == 0 else [result.stderr],
                performance_metrics={"execution_time": execution_time},
                coverage_data={}
            )
            
            self.results.append(test_result)
            self.logger.info(f"âœ… UI/E2E í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {passed_tests}ê°œ ì„±ê³µ, {failed_tests}ê°œ ì‹¤íŒ¨")
            
        except subprocess.TimeoutExpired:
            self.logger.error("âŒ UI/E2E í…ŒìŠ¤íŠ¸ íƒ€ì„ì•„ì›ƒ")
            self.results.append(TestResult(
                suite_name="ui_e2e_tests",
                status="error",
                execution_time=time.time() - start_time,
                total_tests=0,
                passed_tests=0,
                failed_tests=0,
                skipped_tests=0,
                error_details=["UI/E2E test timeout"],
                performance_metrics={},
                coverage_data={}
            ))
            
    async def _monitor_system_metrics(self):
        """ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ëª¨ë‹ˆí„°ë§"""
        interval = self.config.get('system_monitoring', {}).get('interval', 10)
        
        while True:
            try:
                # CPU ì‚¬ìš©ë¥ 
                cpu_usage = psutil.cpu_percent(interval=1)
                
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
                memory = psutil.virtual_memory()
                
                # ë””ìŠ¤í¬ ì‚¬ìš©ë¥ 
                disk = psutil.disk_usage('/')
                
                # ë„¤íŠ¸ì›Œí¬ I/O
                network = psutil.net_io_counters()
                
                # í™œì„± ì—°ê²° ìˆ˜
                connections = len(psutil.net_connections())
                
                metrics = SystemMetrics(
                    timestamp=datetime.now(),
                    cpu_usage=cpu_usage,
                    memory_usage=memory.percent,
                    memory_total=memory.total,
                    disk_usage=disk.percent,
                    network_io={
                        'bytes_sent': network.bytes_sent,
                        'bytes_recv': network.bytes_recv
                    },
                    active_connections=connections
                )
                
                self.system_metrics.append(metrics)
                
                await asyncio.sleep(interval)
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                self.logger.warning(f"ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(interval)
                
    def _parse_pytest_output(self, output: str) -> tuple:
        """Pytest ì¶œë ¥ íŒŒì‹±"""
        import re
        
        # Pytest ê²°ê³¼ íŒ¨í„´ ë§¤ì¹­
        pattern = r'(\d+) passed'
        passed_match = re.search(pattern, output)
        passed_tests = int(passed_match.group(1)) if passed_match else 0
        
        pattern = r'(\d+) failed'
        failed_match = re.search(pattern, output)
        failed_tests = int(failed_match.group(1)) if failed_match else 0
        
        pattern = r'(\d+) skipped'
        skipped_match = re.search(pattern, output)
        skipped_tests = int(skipped_match.group(1)) if skipped_match else 0
        
        return passed_tests, failed_tests, skipped_tests
        
    def _parse_playwright_output(self, output: str) -> tuple:
        """Playwright ì¶œë ¥ íŒŒì‹±"""
        import re
        
        lines = output.split('\n')
        passed_tests = 0
        failed_tests = 0
        skipped_tests = 0
        
        for line in lines:
            if 'âœ“' in line or 'passed' in line.lower():
                passed_tests += 1
            elif 'âœ—' in line or 'failed' in line.lower():
                failed_tests += 1
            elif 'skipped' in line.lower():
                skipped_tests += 1
                
        return passed_tests, failed_tests, skipped_tests
        
    def _load_coverage_data(self, coverage_file: str) -> Dict[str, float]:
        """ì»¤ë²„ë¦¬ì§€ ë°ì´í„° ë¡œë“œ"""
        try:
            with open(coverage_file, 'r') as f:
                coverage_data = json.load(f)
                return {
                    'line_coverage': coverage_data.get('totals', {}).get('percent_covered', 0),
                    'branch_coverage': coverage_data.get('totals', {}).get('percent_covered_display', 0)
                }
        except FileNotFoundError:
            return {'line_coverage': 0, 'branch_coverage': 0}
            
    def _parse_performance_results(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼ íŒŒì‹±"""
        try:
            # ìµœì‹  ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼ íŒŒì¼ ì°¾ê¸°
            import glob
            files = glob.glob('performance_results_*.json')
            if files:
                latest_file = max(files, key=os.path.getctime)
                with open(latest_file, 'r') as f:
                    return json.load(f)
        except Exception:
            pass
            
        return {
            'avg_response_time': 0,
            'p95_response_time': 0,
            'throughput': 0,
            'error_rate': 0
        }
        
    def _parse_security_results(self) -> Dict[str, Any]:
        """ë³´ì•ˆ í…ŒìŠ¤íŠ¸ ê²°ê³¼ íŒŒì‹±"""
        try:
            # ìµœì‹  ë³´ì•ˆ í…ŒìŠ¤íŠ¸ ê²°ê³¼ íŒŒì¼ ì°¾ê¸°
            import glob
            files = glob.glob('comprehensive_security_report_*.json')
            if files:
                latest_file = max(files, key=os.path.getctime)
                with open(latest_file, 'r') as f:
                    data = json.load(f)
                    return {
                        'security_score': data.get('summary', {}).get('overall_security_score', 0),
                        'critical_issues': data.get('summary', {}).get('critical_issues', 0),
                        'high_issues': data.get('summary', {}).get('high_issues', 0)
                    }
        except Exception:
            pass
            
        return {
            'security_score': 0,
            'critical_issues': 0,
            'high_issues': 0
        }
        
    async def _analyze_test_results(self) -> Dict[str, Any]:
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„"""
        self.logger.info("ğŸ“ˆ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„ ì¤‘...")
        
        total_tests = sum(r.total_tests for r in self.results)
        total_passed = sum(r.passed_tests for r in self.results)
        total_failed = sum(r.failed_tests for r in self.results)
        total_skipped = sum(r.skipped_tests for r in self.results)
        
        success_rate = (total_passed / total_tests * 100) if total_tests > 0 else 0
        total_execution_time = sum(r.execution_time for r in self.results)
        
        # ì»¤ë²„ë¦¬ì§€ í†µí•©
        coverage_data = {}
        for result in self.results:
            if result.coverage_data:
                coverage_data.update(result.coverage_data)
                
        # ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ë¶„ì„
        system_analysis = self._analyze_system_metrics()
        
        # ì„±ëŠ¥ ë¶„ì„
        performance_analysis = self._analyze_performance_metrics()
        
        # ë³´ì•ˆ ë¶„ì„
        security_analysis = self._analyze_security_metrics()
        
        analysis = {
            'overall_summary': {
                'total_tests': total_tests,
                'passed_tests': total_passed,
                'failed_tests': total_failed,
                'skipped_tests': total_skipped,
                'success_rate': success_rate,
                'total_execution_time': total_execution_time,
                'test_efficiency': total_tests / total_execution_time if total_execution_time > 0 else 0
            },
            'suite_results': [asdict(r) for r in self.results],
            'coverage_analysis': coverage_data,
            'system_metrics': system_analysis,
            'performance_analysis': performance_analysis,
            'security_analysis': security_analysis,
            'recommendations': self._generate_recommendations()
        }
        
        return analysis
        
    def _analyze_system_metrics(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ë¶„ì„"""
        if not self.system_metrics:
            return {}
            
        cpu_values = [m.cpu_usage for m in self.system_metrics]
        memory_values = [m.memory_usage for m in self.system_metrics]
        
        return {
            'cpu_usage': {
                'avg': sum(cpu_values) / len(cpu_values),
                'max': max(cpu_values),
                'min': min(cpu_values)
            },
            'memory_usage': {
                'avg': sum(memory_values) / len(memory_values),
                'max': max(memory_values),
                'min': min(memory_values)
            },
            'monitoring_duration': len(self.system_metrics) * self.config.get('system_monitoring', {}).get('interval', 10)
        }
        
    def _analyze_performance_metrics(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¶„ì„"""
        performance_results = [r for r in self.results if r.suite_name == 'performance_tests']
        
        if not performance_results:
            return {}
            
        perf_result = performance_results[0]
        metrics = perf_result.performance_metrics
        
        return {
            'response_time_analysis': {
                'avg_response_time': metrics.get('avg_response_time', 0),
                'p95_response_time': metrics.get('p95_response_time', 0),
                'performance_grade': self._get_performance_grade(metrics.get('avg_response_time', 0))
            },
            'throughput_analysis': {
                'requests_per_second': metrics.get('throughput', 0),
                'error_rate': metrics.get('error_rate', 0)
            }
        }
        
    def _analyze_security_metrics(self) -> Dict[str, Any]:
        """ë³´ì•ˆ ë©”íŠ¸ë¦­ ë¶„ì„"""
        security_results = [r for r in self.results if r.suite_name == 'security_tests']
        
        if not security_results:
            return {}
            
        sec_result = security_results[0]
        metrics = sec_result.performance_metrics
        
        return {
            'security_score': metrics.get('security_score', 0),
            'vulnerability_count': {
                'critical': metrics.get('critical_issues', 0),
                'high': metrics.get('high_issues', 0)
            },
            'security_grade': self._get_security_grade(metrics.get('security_score', 0))
        }
        
    def _get_performance_grade(self, avg_response_time: float) -> str:
        """ì„±ëŠ¥ ë“±ê¸‰ ì‚°ì •"""
        if avg_response_time <= 1.0:
            return "A+"
        elif avg_response_time <= 2.0:
            return "A"
        elif avg_response_time <= 3.0:
            return "B"
        elif avg_response_time <= 5.0:
            return "C"
        else:
            return "D"
            
    def _get_security_grade(self, security_score: float) -> str:
        """ë³´ì•ˆ ë“±ê¸‰ ì‚°ì •"""
        if security_score >= 95:
            return "A+"
        elif security_score >= 90:
            return "A"
        elif security_score >= 80:
            return "B"
        elif security_score >= 70:
            return "C"
        else:
            return "D"
            
    def _generate_recommendations(self) -> List[Dict[str, str]]:
        """ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        failed_suites = [r for r in self.results if r.failed_tests > 0]
        for suite in failed_suites:
            recommendations.append({
                'category': 'test_failures',
                'priority': 'high',
                'title': f'{suite.suite_name} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ ìˆ˜ì •',
                'description': f'{suite.failed_tests}ê°œì˜ ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ê°€ ìˆìŠµë‹ˆë‹¤.',
                'action': 'ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ê³  ìˆ˜ì •í•˜ì„¸ìš”.'
            })
            
        # ì„±ëŠ¥ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        performance_results = [r for r in self.results if r.suite_name == 'performance_tests']
        if performance_results:
            perf_metrics = performance_results[0].performance_metrics
            if perf_metrics.get('avg_response_time', 0) > 3.0:
                recommendations.append({
                    'category': 'performance',
                    'priority': 'medium',
                    'title': 'ì‘ë‹µ ì‹œê°„ ìµœì í™”',
                    'description': f'í‰ê·  ì‘ë‹µì‹œê°„ì´ {perf_metrics.get("avg_response_time", 0):.2f}ì´ˆì…ë‹ˆë‹¤.',
                    'action': 'ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ìµœì í™” ë° ìºì‹± ì „ëµì„ ê²€í† í•˜ì„¸ìš”.'
                })
                
        # ì»¤ë²„ë¦¬ì§€ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        total_coverage = 0
        coverage_count = 0
        for result in self.results:
            if result.coverage_data:
                total_coverage += result.coverage_data.get('line_coverage', 0)
                coverage_count += 1
                
        avg_coverage = total_coverage / coverage_count if coverage_count > 0 else 0
        if avg_coverage < 80:
            recommendations.append({
                'category': 'coverage',
                'priority': 'low',
                'title': 'í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ í–¥ìƒ',
                'description': f'í˜„ì¬ ì»¤ë²„ë¦¬ì§€ê°€ {avg_coverage:.1f}%ì…ë‹ˆë‹¤.',
                'action': 'ì¶”ê°€ì ì¸ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¥¼ ì‘ì„±í•˜ì—¬ ì»¤ë²„ë¦¬ì§€ë¥¼ 80% ì´ìƒìœ¼ë¡œ ë†’ì´ì„¸ìš”.'
            })
            
        return recommendations
        
    async def _generate_comprehensive_reports(self, analysis: Dict[str, Any]):
        """ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±"""
        self.logger.info("ğŸ“„ ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # JSON ë¦¬í¬íŠ¸
        json_report = {
            'test_execution': {
                'start_time': self.start_time.isoformat() if self.start_time else None,
                'end_time': self.end_time.isoformat() if self.end_time else None,
                'total_duration': (self.end_time - self.start_time).total_seconds() if self.start_time and self.end_time else 0
            },
            'analysis': analysis,
            'raw_data': {
                'test_results': [asdict(r) for r in self.results],
                'system_metrics': [asdict(m) for m in self.system_metrics]
            }
        }
        
        json_filename = f"comprehensive_test_report_{timestamp}.json"
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump(json_report, f, indent=2, ensure_ascii=False, default=str)
            
        # HTML ë¦¬í¬íŠ¸
        html_report = self._generate_html_report(analysis)
        html_filename = f"comprehensive_test_report_{timestamp}.html"
        with open(html_filename, 'w', encoding='utf-8') as f:
            f.write(html_report)
            
        # JUnit XML ë¦¬í¬íŠ¸
        junit_report = self._generate_junit_report()
        junit_filename = f"comprehensive_test_report_{timestamp}.xml"
        with open(junit_filename, 'w', encoding='utf-8') as f:
            f.write(junit_report)
            
        self.logger.info(f"ğŸ“Š ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ:")
        self.logger.info(f"   JSON: {json_filename}")
        self.logger.info(f"   HTML: {html_filename}")
        self.logger.info(f"   JUnit: {junit_filename}")
        
    def _generate_html_report(self, analysis: Dict[str, Any]) -> str:
        """HTML ë¦¬í¬íŠ¸ ìƒì„±"""
        summary = analysis['overall_summary']
        success_rate = summary['success_rate']
        
        # ì„±ê³µë¥ ì— ë”°ë¥¸ ìƒ‰ìƒ ê²°ì •
        if success_rate >= 95:
            color = "#28a745"  # ë…¹ìƒ‰
        elif success_rate >= 90:
            color = "#ffc107"  # ë…¸ë€ìƒ‰
        else:
            color = "#dc3545"  # ë¹¨ê°„ìƒ‰
            
        html = f"""
<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI ëª¨ë¸ ê´€ë¦¬ ì‹œìŠ¤í…œ - í†µí•© í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸</title>
    <style>
        body {{ font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; margin: 0; padding: 20px; background: #f5f7fa; }}
        .container {{ max-width: 1400px; margin: 0 auto; }}
        .header {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 30px; border-radius: 10px; text-align: center; margin-bottom: 30px; }}
        .success-rate {{ font-size: 72px; font-weight: bold; color: {color}; margin: 20px 0; }}
        .grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; margin-bottom: 30px; }}
        .card {{ background: white; border-radius: 10px; padding: 20px; box-shadow: 0 4px 6px rgba(0,0,0,0.1); }}
        .metric {{ text-align: center; }}
        .metric-value {{ font-size: 36px; font-weight: bold; margin: 10px 0; }}
        .metric-label {{ color: #666; font-size: 14px; }}
        .suite-result {{ margin: 10px 0; padding: 15px; border-radius: 5px; }}
        .suite-success {{ background: #d4edda; border-left: 4px solid #28a745; }}
        .suite-failure {{ background: #f8d7da; border-left: 4px solid #dc3545; }}
        .suite-error {{ background: #fff3cd; border-left: 4px solid #ffc107; }}
        .recommendations {{ background: #e7f3ff; border: 1px solid #b3d9ff; padding: 20px; border-radius: 10px; }}
        .chart-container {{ position: relative; height: 300px; margin: 20px 0; }}
    </style>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ”§ AI ëª¨ë¸ ê´€ë¦¬ ì‹œìŠ¤í…œ</h1>
            <h2>í†µí•© í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸</h2>
            <p>ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        </div>
        
        <div class="card" style="text-align: center; margin-bottom: 30px;">
            <h2>ì „ì²´ ì„±ê³µë¥ </h2>
            <div class="success-rate">{success_rate:.1f}%</div>
            <p>ì´ {summary['total_tests']}ê°œ í…ŒìŠ¤íŠ¸ ì¤‘ {summary['passed_tests']}ê°œ ì„±ê³µ</p>
        </div>
        
        <div class="grid">
            <div class="card metric">
                <div class="metric-value" style="color: #28a745;">{summary['passed_tests']}</div>
                <div class="metric-label">ì„±ê³µí•œ í…ŒìŠ¤íŠ¸</div>
            </div>
            <div class="card metric">
                <div class="metric-value" style="color: #dc3545;">{summary['failed_tests']}</div>
                <div class="metric-label">ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸</div>
            </div>
            <div class="card metric">
                <div class="metric-value" style="color: #ffc107;">{summary['skipped_tests']}</div>
                <div class="metric-label">ê±´ë„ˆë›´ í…ŒìŠ¤íŠ¸</div>
            </div>
            <div class="card metric">
                <div class="metric-value" style="color: #6c757d;">{summary['total_execution_time']:.1f}s</div>
                <div class="metric-label">ì´ ì‹¤í–‰ ì‹œê°„</div>
            </div>
        </div>
        
        <div class="card">
            <h3>ğŸ“‹ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ë³„ ê²°ê³¼</h3>
        """
        
        # í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ê²°ê³¼
        for result in self.results:
            status_class = "suite-success" if result.status == "success" else "suite-failure" if result.status == "failure" else "suite-error"
            status_icon = "âœ…" if result.status == "success" else "âŒ" if result.status == "failure" else "âš ï¸"
            
            html += f"""
            <div class="suite-result {status_class}">
                <h4>{status_icon} {result.suite_name}</h4>
                <p>ì„±ê³µ: {result.passed_tests} | ì‹¤íŒ¨: {result.failed_tests} | ì‹¤í–‰ì‹œê°„: {result.execution_time:.2f}ì´ˆ</p>
            </div>
            """
            
        html += """
        </div>
        
        <div class="grid">
        """
        
        # ì„±ëŠ¥ ë¶„ì„
        perf_analysis = analysis.get('performance_analysis', {})
        if perf_analysis:
            response_time = perf_analysis.get('response_time_analysis', {})
            html += f"""
            <div class="card">
                <h3>ğŸ“Š ì„±ëŠ¥ ë¶„ì„</h3>
                <p><strong>í‰ê·  ì‘ë‹µì‹œê°„:</strong> {response_time.get('avg_response_time', 0):.2f}ì´ˆ</p>
                <p><strong>95% ì‘ë‹µì‹œê°„:</strong> {response_time.get('p95_response_time', 0):.2f}ì´ˆ</p>
                <p><strong>ì„±ëŠ¥ ë“±ê¸‰:</strong> {response_time.get('performance_grade', 'N/A')}</p>
            </div>
            """
            
        # ë³´ì•ˆ ë¶„ì„
        sec_analysis = analysis.get('security_analysis', {})
        if sec_analysis:
            html += f"""
            <div class="card">
                <h3>ğŸ”’ ë³´ì•ˆ ë¶„ì„</h3>
                <p><strong>ë³´ì•ˆ ì ìˆ˜:</strong> {sec_analysis.get('security_score', 0)}/100</p>
                <p><strong>ì¹˜ëª…ì  ì·¨ì•½ì :</strong> {sec_analysis.get('vulnerability_count', {}).get('critical', 0)}ê°œ</p>
                <p><strong>ë³´ì•ˆ ë“±ê¸‰:</strong> {sec_analysis.get('security_grade', 'N/A')}</p>
            </div>
            """
            
        html += """
        </div>
        
        <div class="recommendations">
            <h3>ğŸ’¡ ê°œì„  ê¶Œì¥ì‚¬í•­</h3>
        """
        
        # ê¶Œì¥ì‚¬í•­
        recommendations = analysis.get('recommendations', [])
        for i, rec in enumerate(recommendations[:5], 1):
            priority_color = "#dc3545" if rec['priority'] == 'high' else "#ffc107" if rec['priority'] == 'medium' else "#28a745"
            html += f"""
            <div style="margin: 15px 0; padding: 10px; border-left: 4px solid {priority_color}; background: rgba(255,255,255,0.5);">
                <h4>[{rec['priority'].upper()}] {rec['title']}</h4>
                <p>{rec['description']}</p>
                <p><strong>ì¡°ì¹˜:</strong> {rec['action']}</p>
            </div>
            """
            
        html += """
        </div>
        
        <div style="text-align: center; margin-top: 40px; color: #666;">
            <p>ì´ ë¦¬í¬íŠ¸ëŠ” AI ëª¨ë¸ ê´€ë¦¬ ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸ ë„êµ¬ì— ì˜í•´ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.</p>
        </div>
    </div>
</body>
</html>
        """
        
        return html
        
    def _generate_junit_report(self) -> str:
        """JUnit XML ë¦¬í¬íŠ¸ ìƒì„±"""
        total_tests = sum(r.total_tests for r in self.results)
        total_failures = sum(r.failed_tests for r in self.results)
        total_time = sum(r.execution_time for r in self.results)
        
        xml = f"""<?xml version="1.0" encoding="UTF-8"?>
<testsuite name="AI Model Management Integration Tests" 
           tests="{total_tests}" 
           failures="{total_failures}" 
           time="{total_time:.2f}">
"""
        
        for result in self.results:
            xml += f"""  <testcase name="{result.suite_name}" 
                    classname="IntegrationTest" 
                    time="{result.execution_time:.2f}">"""
            
            if result.failed_tests > 0:
                xml += f"""
    <failure message="Test failures detected">
      Failed tests: {result.failed_tests}
      Error details: {'; '.join(result.error_details)}
    </failure>"""
                    
            xml += """
  </testcase>"""
            
        xml += """
</testsuite>"""
        
        return xml

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    orchestrator = ComprehensiveTestOrchestrator()
    
    try:
        results = await orchestrator.run_comprehensive_tests()
        
        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        summary = results['overall_summary']
        print(f"\n{'='*60}")
        print(f"ğŸ¯ AI ëª¨ë¸ ê´€ë¦¬ ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        print(f"{'='*60}")
        print(f"ğŸ“Š ì „ì²´ ì„±ê³µë¥ : {summary['success_rate']:.1f}%")
        print(f"âœ… ì„±ê³µí•œ í…ŒìŠ¤íŠ¸: {summary['passed_tests']}ê°œ")
        print(f"âŒ ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸: {summary['failed_tests']}ê°œ")
        print(f"â±ï¸ ì´ ì‹¤í–‰ ì‹œê°„: {summary['total_execution_time']:.1f}ì´ˆ")
        
        # ì‹¤íŒ¨ ì‹œ ì¢…ë£Œ ì½”ë“œ 1 ë°˜í™˜
        if summary['failed_tests'] > 0:
            sys.exit(1)
            
    except Exception as e:
        print(f"âŒ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())