#!/usr/bin/env python3
"""
ELK Stack ì„±ëŠ¥ ë° ìƒíƒœ ëª¨ë‹ˆí„°ë§ ìŠ¤í¬ë¦½íŠ¸
Online Evaluation System - ELK Stack Performance Monitor
"""

import requests
import json
import time
import sys
from datetime import datetime, timedelta
from typing import Dict, List, Any
import asyncio
import aiohttp

class ELKPerformanceMonitor:
    def __init__(self):
        self.elasticsearch_url = "http://localhost:9200"
        self.kibana_url = "http://localhost:5601"
        self.logstash_url = "http://localhost:9600"
        
    async def check_elasticsearch_health(self) -> Dict[str, Any]:
        """Elasticsearch í´ëŸ¬ìŠ¤í„° ìƒíƒœ í™•ì¸"""
        try:
            async with aiohttp.ClientSession() as session:
                # í´ëŸ¬ìŠ¤í„° ìƒíƒœ
                async with session.get(f"{self.elasticsearch_url}/_cluster/health") as resp:
                    cluster_health = await resp.json()
                
                # ë…¸ë“œ í†µê³„
                async with session.get(f"{self.elasticsearch_url}/_nodes/stats") as resp:
                    node_stats = await resp.json()
                
                # ì¸ë±ìŠ¤ í†µê³„
                async with session.get(f"{self.elasticsearch_url}/_cat/indices?format=json") as resp:
                    indices_stats = await resp.json()
                
                return {
                    "status": "healthy" if cluster_health["status"] in ["green", "yellow"] else "unhealthy",
                    "cluster_health": cluster_health,
                    "node_stats": node_stats,
                    "indices_stats": indices_stats,
                    "timestamp": datetime.utcnow().isoformat()
                }
        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "timestamp": datetime.utcnow().isoformat()
            }
    
    async def check_logstash_health(self) -> Dict[str, Any]:
        """Logstash ìƒíƒœ ë° ì„±ëŠ¥ í™•ì¸"""
        try:
            async with aiohttp.ClientSession() as session:
                # Logstash ìƒíƒœ
                async with session.get(f"{self.logstash_url}") as resp:
                    logstash_status = await resp.json()
                
                # íŒŒì´í”„ë¼ì¸ í†µê³„
                async with session.get(f"{self.logstash_url}/_node/stats/pipelines") as resp:
                    pipeline_stats = await resp.json()
                
                return {
                    "status": "healthy" if logstash_status.get("status") == "green" else "unhealthy",
                    "logstash_status": logstash_status,
                    "pipeline_stats": pipeline_stats,
                    "timestamp": datetime.utcnow().isoformat()
                }
        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "timestamp": datetime.utcnow().isoformat()
            }
    
    async def check_kibana_health(self) -> Dict[str, Any]:
        """Kibana ìƒíƒœ í™•ì¸"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(f"{self.kibana_url}/api/status") as resp:
                    kibana_status = await resp.json()
                
                return {
                    "status": "healthy" if kibana_status.get("status", {}).get("overall", {}).get("state") == "green" else "unhealthy",
                    "kibana_status": kibana_status,
                    "timestamp": datetime.utcnow().isoformat()
                }
        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "timestamp": datetime.utcnow().isoformat()
            }
    
    async def analyze_log_patterns(self) -> Dict[str, Any]:
        """ë¡œê·¸ íŒ¨í„´ ë¶„ì„"""
        try:
            async with aiohttp.ClientSession() as session:
                # ìµœê·¼ 1ì‹œê°„ ë¡œê·¸ ë¶„ì„
                query = {
                    "query": {
                        "range": {
                            "@timestamp": {
                                "gte": "now-1h"
                            }
                        }
                    },
                    "aggs": {
                        "log_levels": {
                            "terms": {
                                "field": "level",
                                "size": 10
                            }
                        },
                        "error_patterns": {
                            "filter": {
                                "term": {
                                    "level": "ERROR"
                                }
                            },
                            "aggs": {
                                "top_errors": {
                                    "terms": {
                                        "field": "error.type",
                                        "size": 5
                                    }
                                }
                            }
                        },
                        "performance_metrics": {
                            "stats": {
                                "field": "performance.duration"
                            }
                        },
                        "user_activity": {
                            "cardinality": {
                                "field": "user_id"
                            }
                        }
                    },
                    "size": 0
                }
                
                async with session.post(
                    f"{self.elasticsearch_url}/app-logs-*/_search",
                    json=query,
                    headers={"Content-Type": "application/json"}
                ) as resp:
                    search_results = await resp.json()
                
                return {
                    "status": "success",
                    "analysis": search_results.get("aggregations", {}),
                    "total_logs": search_results.get("hits", {}).get("total", {}).get("value", 0),
                    "timestamp": datetime.utcnow().isoformat()
                }
        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "timestamp": datetime.utcnow().isoformat()
            }
    
    async def check_index_performance(self) -> Dict[str, Any]:
        """ì¸ë±ìŠ¤ ì„±ëŠ¥ ë¶„ì„"""
        try:
            async with aiohttp.ClientSession() as session:
                # ì¸ë±ìŠ¤ í¬ê¸° ë° ì„±ëŠ¥ í†µê³„
                async with session.get(f"{self.elasticsearch_url}/_cat/indices/app-logs-*?format=json&bytes=b") as resp:
                    indices_info = await resp.json()
                
                # ìƒ¤ë“œ ì •ë³´
                async with session.get(f"{self.elasticsearch_url}/_cat/shards/app-logs-*?format=json") as resp:
                    shards_info = await resp.json()
                
                performance_summary = {
                    "total_indices": len(indices_info),
                    "total_size_bytes": sum(int(idx.get("store.size", "0")) for idx in indices_info),
                    "total_docs": sum(int(idx.get("docs.count", "0")) for idx in indices_info),
                    "largest_index": max(indices_info, key=lambda x: int(x.get("store.size", "0")), default={}),
                    "indices_health": [
                        {
                            "index": idx["index"],
                            "health": idx["health"],
                            "size": idx.get("store.size", "0"),
                            "docs": idx.get("docs.count", "0")
                        }
                        for idx in indices_info
                    ]
                }
                
                return {
                    "status": "success",
                    "performance_summary": performance_summary,
                    "indices_details": indices_info,
                    "shards_details": shards_info,
                    "timestamp": datetime.utcnow().isoformat()
                }
        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "timestamp": datetime.utcnow().isoformat()
            }
    
    async def generate_performance_report(self) -> Dict[str, Any]:
        """ì¢…í•© ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±"""
        print("ğŸ” ELK Stack ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘...")
        
        # ëª¨ë“  ê²€ì‚¬ë¥¼ ë³‘ë ¬ë¡œ ì‹¤í–‰
        tasks = [
            self.check_elasticsearch_health(),
            self.check_logstash_health(),
            self.check_kibana_health(),
            self.analyze_log_patterns(),
            self.check_index_performance()
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        elasticsearch_health, logstash_health, kibana_health, log_analysis, index_performance = results
        
        # ì „ì²´ ì‹œìŠ¤í…œ ìƒíƒœ í‰ê°€
        overall_status = "healthy"
        if any(result.get("status") != "healthy" and result.get("status") != "success" 
               for result in results if isinstance(result, dict)):
            overall_status = "degraded"
        
        # ê¶Œì¥ì‚¬í•­ ìƒì„±
        recommendations = []
        
        if isinstance(elasticsearch_health, dict) and elasticsearch_health.get("status") == "healthy":
            cluster_health = elasticsearch_health.get("cluster_health", {})
            if cluster_health.get("number_of_pending_tasks", 0) > 0:
                recommendations.append("Elasticsearch has pending tasks - consider scaling resources")
        
        if isinstance(index_performance, dict) and index_performance.get("status") == "success":
            perf_summary = index_performance.get("performance_summary", {})
            if perf_summary.get("total_size_bytes", 0) > 50 * 1024 * 1024 * 1024:  # 50GB
                recommendations.append("Index size is large - consider implementing ILM policies")
        
        if isinstance(log_analysis, dict) and log_analysis.get("status") == "success":
            analysis = log_analysis.get("analysis", {})
            error_count = 0
            for bucket in analysis.get("log_levels", {}).get("buckets", []):
                if bucket.get("key") == "ERROR":
                    error_count = bucket.get("doc_count", 0)
            if error_count > 100:  # 1ì‹œê°„ì— 100ê°œ ì´ìƒ ì—ëŸ¬
                recommendations.append(f"High error rate detected: {error_count} errors in the last hour")
        
        report = {
            "overview": {
                "timestamp": datetime.utcnow().isoformat(),
                "overall_status": overall_status,
                "recommendations": recommendations
            },
            "elasticsearch": elasticsearch_health if isinstance(elasticsearch_health, dict) else {"status": "error", "error": str(elasticsearch_health)},
            "logstash": logstash_health if isinstance(logstash_health, dict) else {"status": "error", "error": str(logstash_health)},
            "kibana": kibana_health if isinstance(kibana_health, dict) else {"status": "error", "error": str(kibana_health)},
            "log_analysis": log_analysis if isinstance(log_analysis, dict) else {"status": "error", "error": str(log_analysis)},
            "index_performance": index_performance if isinstance(index_performance, dict) else {"status": "error", "error": str(index_performance)}
        }
        
        return report
    
    def print_performance_summary(self, report: Dict[str, Any]):
        """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìš”ì•½ ì¶œë ¥"""
        print("\\n" + "="*80)
        print("ğŸ¯ ELK STACK ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë¦¬í¬íŠ¸")
        print("="*80)
        
        overview = report.get("overview", {})
        print(f"â° ì‹¤í–‰ ì‹œê°„: {overview.get('timestamp', 'N/A')}")
        print(f"ğŸ¥ ì „ì²´ ìƒíƒœ: {overview.get('overall_status', 'unknown').upper()}")
        
        # ì„œë¹„ìŠ¤ë³„ ìƒíƒœ
        print("\\nğŸ“Š ì„œë¹„ìŠ¤ ìƒíƒœ:")
        services = ["elasticsearch", "logstash", "kibana"]
        for service in services:
            service_data = report.get(service, {})
            status = service_data.get("status", "unknown")
            emoji = "âœ…" if status in ["healthy", "success"] else "âŒ" if status == "error" else "âš ï¸"
            print(f"  {emoji} {service.title()}: {status}")
        
        # ë¡œê·¸ ë¶„ì„ ìš”ì•½
        log_analysis = report.get("log_analysis", {})
        if log_analysis.get("status") == "success":
            print("\\nğŸ“ˆ ë¡œê·¸ ë¶„ì„ (ìµœê·¼ 1ì‹œê°„):")
            print(f"  ğŸ“„ ì´ ë¡œê·¸ ìˆ˜: {log_analysis.get('total_logs', 0):,}")
            
            analysis = log_analysis.get("analysis", {})
            if "log_levels" in analysis:
                print("  ğŸ“Š ë¡œê·¸ ë ˆë²¨ ë¶„í¬:")
                for bucket in analysis["log_levels"].get("buckets", []):
                    level = bucket.get("key", "unknown")
                    count = bucket.get("doc_count", 0)
                    print(f"    â€¢ {level}: {count:,}")
            
            if "user_activity" in analysis:
                active_users = analysis["user_activity"].get("value", 0)
                print(f"  ğŸ‘¥ í™œì„± ì‚¬ìš©ì: {active_users}")
        
        # ì¸ë±ìŠ¤ ì„±ëŠ¥ ìš”ì•½
        index_perf = report.get("index_performance", {})
        if index_perf.get("status") == "success":
            perf_summary = index_perf.get("performance_summary", {})
            print("\\nğŸ’¾ ì¸ë±ìŠ¤ ì„±ëŠ¥:")
            print(f"  ğŸ“ ì´ ì¸ë±ìŠ¤ ìˆ˜: {perf_summary.get('total_indices', 0)}")
            print(f"  ğŸ“„ ì´ ë¬¸ì„œ ìˆ˜: {perf_summary.get('total_docs', 0):,}")
            total_size_mb = perf_summary.get('total_size_bytes', 0) / (1024 * 1024)
            print(f"  ğŸ’¿ ì´ í¬ê¸°: {total_size_mb:.2f} MB")
        
        # ê¶Œì¥ì‚¬í•­
        recommendations = overview.get("recommendations", [])
        if recommendations:
            print("\\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
            for i, rec in enumerate(recommendations, 1):
                print(f"  {i}. {rec}")
        
        print("\\n" + "="*80)

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    monitor = ELKPerformanceMonitor()
    
    if len(sys.argv) > 1 and sys.argv[1] == "--watch":
        # ì§€ì†ì ì¸ ëª¨ë‹ˆí„°ë§ ëª¨ë“œ
        print("ğŸ”„ ì§€ì†ì ì¸ ëª¨ë‹ˆí„°ë§ ëª¨ë“œ ì‹œì‘ (Ctrl+Cë¡œ ì¢…ë£Œ)")
        try:
            while True:
                report = await monitor.generate_performance_report()
                monitor.print_performance_summary(report)
                print("\\nâ³ 5ë¶„ í›„ ë‹¤ì‹œ í™•ì¸...")
                await asyncio.sleep(300)  # 5ë¶„ ëŒ€ê¸°
        except KeyboardInterrupt:
            print("\\nğŸ‘‹ ëª¨ë‹ˆí„°ë§ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    else:
        # ì¼íšŒì„± ê²€ì‚¬
        report = await monitor.generate_performance_report()
        monitor.print_performance_summary(report)
        
        # JSON ë¦¬í¬íŠ¸ ì €ì¥
        report_file = f"elk_performance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        print(f"\\nğŸ“ ìƒì„¸ ë¦¬í¬íŠ¸ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {report_file}")

if __name__ == "__main__":
    asyncio.run(main())
