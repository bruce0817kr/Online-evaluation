# =================== Logstash Filters Configuration ===================
#
# Comprehensive filtering and parsing rules for different log types
# This file contains all the transformation logic

filter {
  # ============================= COMMON PREPROCESSING =============================
  
  # Add processing metadata
  mutate {
    add_field => { 
      "[@metadata][pipeline_version]" => "1.0"
      "[@metadata][processed_by]" => "logstash"
      "[@metadata][processing_timestamp]" => "%{+YYYY-MM-dd'T'HH:mm:ss.SSSZ}"
    }
  }
  
  # Identify log source from beats metadata
  if [fields][log_type] {
    mutate {
      copy => { "[fields][log_type]" => "log_source" }
    }
  } else if [container][name] {
    mutate {
      add_field => { "log_source" => "docker" }
    }
  } else {
    mutate {
      add_field => { "log_source" => "unknown" }
    }
  }
  
  # ============================= APPLICATION LOGS =============================
  
  if [log_source] == "application" or [fields][service] == "fastapi" {
    # Handle JSON formatted application logs
    if [message] =~ /^\s*\{.*\}\s*$/ {
      json {
        source => "message"
        target => "app"
        add_tag => ["json_parsed"]
      }
      
      if [app] {
        # Extract standard logging fields
        if [app][timestamp] {
          mutate { copy => { "[app][timestamp]" => "log_timestamp" } }
        }
        if [app][level] or [app][levelname] {
          mutate { 
            copy => { 
              "[app][level]" => "level" 
              "[app][levelname]" => "level"
            } 
          }
        }
        if [app][name] or [app][logger] {
          mutate { 
            copy => { 
              "[app][name]" => "logger" 
              "[app][logger]" => "logger"
            } 
          }
        }
        if [app][message] or [app][msg] {
          mutate { 
            copy => { 
              "[app][message]" => "log_message" 
              "[app][msg]" => "log_message"
            } 
          }
        }
        
        # Extract HTTP request information
        if [app][http] {
          mutate {
            add_field => {
              "http_method" => "%{[app][http][method]}"
              "http_url" => "%{[app][http][url]}"
              "http_status_code" => "%{[app][http][status_code]}"
              "http_response_time" => "%{[app][http][response_time]}"
              "http_user_agent" => "%{[app][http][user_agent]}"
              "remote_addr" => "%{[app][http][remote_addr]}"
            }
          }
          
          # Convert numeric fields
          if [app][http][status_code] {
            mutate {
              convert => { "http_status_code" => "integer" }
            }
          }
          if [app][http][response_time] {
            mutate {
              convert => { "http_response_time" => "float" }
            }
          }
        }
        
        # Extract error information
        if [app][error] {
          mutate {
            add_field => {
              "error_type" => "%{[app][error][type]}"
              "error_message" => "%{[app][error][message]}"
              "error_stack_trace" => "%{[app][error][stack_trace]}"
            }
            add_tag => ["error_log"]
          }
        }
        
        # Extract user context
        if [app][user_id] {
          mutate { copy => { "[app][user_id]" => "user_id" } }
        }
        if [app][session_id] {
          mutate { copy => { "[app][session_id]" => "session_id" } }
        }
        if [app][request_id] {
          mutate { copy => { "[app][request_id]" => "request_id" } }
        }
        
        # Clean up - remove the nested app object to save space
        mutate {
          remove_field => ["app"]
        }
      } else {
        mutate {
          add_tag => ["json_parse_failed"]
        }
      }
    } else {
      # Handle plain text logs with grok patterns
      grok {
        match => { 
          "message" => [
            "%{TIMESTAMP_ISO8601:log_timestamp} - %{LOGLEVEL:level} - %{DATA:logger} - %{GREEDYDATA:log_message}",
            "%{TIMESTAMP_ISO8601:log_timestamp} - %{LOGLEVEL:level} - %{GREEDYDATA:log_message}",
            "%{TIMESTAMP_ISO8601:log_timestamp} %{LOGLEVEL:level} %{GREEDYDATA:log_message}"
          ]
        }
        tag_on_failure => ["_grokparsefailure_application"]
      }
    }
    
    # Set metadata for indexing
    mutate {
      add_field => { "[@metadata][index_prefix]" => "app-logs" }
      add_field => { "service_type" => "application" }
    }
  }
  
  # ============================= NGINX LOGS =============================
  
  else if [log_source] == "nginx" {
    if [fields][nginx_log_type] == "access" {
      # Parse Nginx access logs
      grok {
        match => { 
          "message" => '%{IPORHOST:remote_addr} - %{USER:remote_user} \[%{HTTPDATE:log_timestamp}\] "%{WORD:http_method} %{URIPATHPARAM:http_url} HTTP/%{NUMBER:http_version}" %{INT:http_status_code} %{INT:body_bytes_sent} "%{DATA:http_referer}" "%{DATA:http_user_agent}" %{NUMBER:request_time:float}'
        }
        tag_on_failure => ["_grokparsefailure_nginx_access"]
      }
      
      if !("_grokparsefailure_nginx_access" in [tags]) {
        # Convert field types
        mutate {
          convert => {
            "http_status_code" => "integer"
            "body_bytes_sent" => "integer"
            "request_time" => "float"
          }
          add_field => { "log_type" => "access" }
        }
        
        # Categorize HTTP status codes
        if [http_status_code] {
          if [http_status_code] >= 200 and [http_status_code] < 300 {
            mutate { add_tag => ["success"] }
          } else if [http_status_code] >= 300 and [http_status_code] < 400 {
            mutate { add_tag => ["redirect"] }
          } else if [http_status_code] >= 400 and [http_status_code] < 500 {
            mutate { add_tag => ["client_error"] }
          } else if [http_status_code] >= 500 {
            mutate { add_tag => ["server_error"] }
          }
        }
      }
    } else if [fields][nginx_log_type] == "error" {
      # Parse Nginx error logs
      grok {
        match => { 
          "message" => '%{TIMESTAMP_ISO8601:log_timestamp} \[%{LOGLEVEL:level}\] %{POSINT:pid}#%{NUMBER:tid}: (\*%{NUMBER:connection_id} )?%{GREEDYDATA:error_message}'
        }
        tag_on_failure => ["_grokparsefailure_nginx_error"]
      }
      
      if !("_grokparsefailure_nginx_error" in [tags]) {
        mutate {
          add_field => { "log_type" => "error" }
          add_tag => ["nginx_error"]
        }
      }
    }
    
    # Set metadata for indexing
    mutate {
      add_field => { "[@metadata][index_prefix]" => "nginx-logs" }
      add_field => { "service_type" => "nginx" }
    }
  }
  
  # ============================= DOCKER LOGS =============================
  
  else if [log_source] == "docker" or [container] {
    # Extract container information
    if [container][name] {
      mutate {
        copy => { "[container][name]" => "container_name" }
      }
    }
    if [container][id] {
      mutate {
        copy => { "[container][id]" => "container_id" }
      }
    }
    if [container][image] {
      mutate {
        copy => { "[container][image]" => "container_image" }
      }
    }
    
    # Try to parse the log message as JSON (for structured container logs)
    if [message] =~ /^\s*\{.*\}\s*$/ {
      json {
        source => "message"
        target => "container_log"
        add_tag => ["container_json_parsed"]
      }
      
      if [container_log] {
        # Extract common fields from container JSON logs
        if [container_log][level] {
          mutate { copy => { "[container_log][level]" => "level" } }
        }
        if [container_log][message] {
          mutate { copy => { "[container_log][message]" => "log_message" } }
        }
        if [container_log][timestamp] {
          mutate { copy => { "[container_log][timestamp]" => "log_timestamp" } }
        }
      }
    } else {
      # For plain text container logs, just copy the message
      mutate {
        copy => { "message" => "log_message" }
      }
    }
    
    # Set metadata for indexing
    mutate {
      add_field => { "[@metadata][index_prefix]" => "docker-logs" }
      add_field => { "service_type" => "docker" }
    }
  }
  
  # ============================= UNKNOWN LOGS =============================
  
  else {
    # Generic processing for unknown log sources
    mutate {
      add_field => { 
        "[@metadata][index_prefix]" => "misc-logs" 
        "service_type" => "unknown"
      }
      add_tag => ["unknown_source"]
      copy => { "message" => "log_message" }
    }
  }
  
  # ============================= COMMON POST-PROCESSING =============================
  
  # Parse timestamp fields
  if [log_timestamp] {
    date {
      match => [ 
        "log_timestamp", 
        "ISO8601",
        "yyyy-MM-dd HH:mm:ss,SSS",
        "yyyy-MM-dd'T'HH:mm:ss.SSSZ",
        "dd/MMM/yyyy:HH:mm:ss Z",
        "yyyy-MM-dd HH:mm:ss.SSS"
      ]
      target => "@timestamp"
      tag_on_failure => ["_dateparsefailure"]
    }
    
    # Remove the original timestamp field after parsing
    mutate {
      remove_field => ["log_timestamp"]
    }
  }
  
  # GeoIP enrichment for external IP addresses
  if [remote_addr] and [remote_addr] !~ /^(10\.|172\.(1[6-9]|2[0-9]|3[0-1])\.|192\.168\.|127\.|::1|localhost)/ {
    geoip {
      source => "remote_addr"
      target => "geoip"
      add_tag => ["geoip_enriched"]
    }
  }
  
  # Security: Remove sensitive fields to prevent data leaks
  mutate {
    remove_field => [
      "password", "passwd", "pwd",
      "token", "jwt", "bearer",
      "secret", "key", "apikey", "api_key",
      "auth", "authorization",
      "credential", "cred"
    ]
  }
  
  # Normalize log levels
  if [level] {
    mutate {
      uppercase => ["level"]
    }
  }
  
  # Add processing statistics
  mutate {
    add_field => { 
      "[@metadata][processing_time]" => "%{+UNIX_MS}"
      "[@metadata][logstash_host]" => "%{[host][name]}"
    }
  }
}
