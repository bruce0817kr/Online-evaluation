# =================== Logstash Main Pipeline Configuration ===================
#
# This is the main pipeline configuration for the Online Evaluation System
# It processes logs from various sources and forwards them to Elasticsearch
#
# Pipeline Flow:
# Input Sources -> Filters -> Output Destinations
#

# =================================== INPUTS ===================================

input {
  # Beats input - receives logs from Filebeat
  beats {
    port => 5044
    host => "0.0.0.0"
    
    # Optional SSL configuration (disabled for development)
    # ssl => true
    # ssl_certificate => "/etc/logstash/ssl/logstash.crt"
    # ssl_key => "/etc/logstash/ssl/logstash.key"
  }
  
  # HTTP input - for direct log submission
  http {
    port => 8080
    host => "0.0.0.0"
    codec => json
    
    # Add source identification
    add_field => { "input_type" => "http" }
  }
  
  # Docker logs input (if using Docker log driver)
  # tcp {
  #   port => 5000
  #   codec => json
  #   add_field => { "input_type" => "docker" }
  # }
}

# ================================== FILTERS ==================================

filter {
  # Add common fields
  mutate {
    add_field => { 
      "[@metadata][processed_at]" => "%{+YYYY-MM-dd'T'HH:mm:ss.SSSZ}"
      "logstash_host" => "%{[host][name]}"
    }
  }
  
  # Process based on input source
  if [fields][log_type] {
    mutate {
      add_field => { "log_source" => "%{[fields][log_type]}" }
    }
  }
  
  # Route to specific filter configurations based on source
  if [log_source] == "application" or [fields][service] == "backend" {
    # Application logs processing
    if [message] =~ /^\{.*\}$/ {
      # Parse JSON logs
      json {
        source => "message"
        target => "parsed"
      }
      
      # Extract common fields from parsed JSON
      if [parsed] {
        mutate {
          add_field => {
            "level" => "%{[parsed][level]}"
            "logger" => "%{[parsed][name]}"
            "module" => "%{[parsed][module]}"
          }
        }
        
        # Handle HTTP request logs
        if [parsed][http] {
          mutate {
            add_field => {
              "http_method" => "%{[parsed][http][method]}"
              "http_url" => "%{[parsed][http][url]}"
              "http_status" => "%{[parsed][http][status_code]}"
              "response_time" => "%{[parsed][http][response_time]}"
            }
          }
        }
        
        # Handle error logs
        if [parsed][error] {
          mutate {
            add_field => {
              "error_type" => "%{[parsed][error][type]}"
              "error_message" => "%{[parsed][error][message]}"
            }
          }
        }
      }
    } else {
      # Parse plain text logs with grok patterns
      grok {
        match => { 
          "message" => "%{TIMESTAMP_ISO8601:timestamp} - %{LOGLEVEL:level} - %{GREEDYDATA:log_message}" 
        }
        tag_on_failure => ["_grokparsefailure_application"]
      }
    }
    
    # Set index pattern for application logs
    mutate {
      add_field => { "[@metadata][index_prefix]" => "app-logs" }
    }
  }
  
  else if [log_source] == "nginx" or [fields][service] == "nginx" {
    # Nginx logs processing
    if [fields][nginx_log_type] == "access" {
      # Parse Nginx access logs
      grok {
        match => { 
          "message" => '%{IPORHOST:remote_addr} - %{USER:remote_user} \[%{HTTPDATE:time_local}\] "%{WORD:method} %{URIPATHPARAM:request} HTTP/%{NUMBER:http_version}" %{INT:status} %{INT:body_bytes_sent} "%{DATA:http_referer}" "%{DATA:http_user_agent}" %{NUMBER:request_time}' 
        }
        tag_on_failure => ["_grokparsefailure_nginx_access"]
      }
      
      # Convert fields to appropriate types
      mutate {
        convert => {
          "status" => "integer"
          "body_bytes_sent" => "integer"
          "request_time" => "float"
        }
      }
    } else {
      # Parse Nginx error logs
      grok {
        match => { 
          "message" => '%{TIMESTAMP_ISO8601:timestamp} \[%{LOGLEVEL:error_level}\] %{POSINT:pid}#%{NUMBER:tid}: (\*%{NUMBER:connection_id} )?%{GREEDYDATA:error_message}' 
        }
        tag_on_failure => ["_grokparsefailure_nginx_error"]
      }
    }
    
    # Set index pattern for nginx logs
    mutate {
      add_field => { "[@metadata][index_prefix]" => "nginx-logs" }
    }
  }
  
  else if [log_source] == "docker" or [container] {
    # Docker container logs processing
    if [container][name] {
      mutate {
        add_field => { "service_name" => "%{[container][name]}" }
      }
    }
    
    # Try to parse container logs as JSON first
    if [message] =~ /^\{.*\}$/ {
      json {
        source => "message"
        target => "container_log"
      }
    }
    
    # Set index pattern for docker logs
    mutate {
      add_field => { "[@metadata][index_prefix]" => "docker-logs" }
    }
  }
  
  else {
    # Generic log processing for unknown sources
    mutate {
      add_field => { "[@metadata][index_prefix]" => "misc-logs" }
      add_tag => ["unknown_source"]
    }
  }
  
  # Parse timestamp if present
  if [timestamp] {
    date {
      match => [ 
        "timestamp", 
        "ISO8601",
        "yyyy-MM-dd HH:mm:ss,SSS",
        "yyyy-MM-dd'T'HH:mm:ss.SSSZ",
        "dd/MMM/yyyy:HH:mm:ss Z"
      ]
      target => "@timestamp"
    }
  }
  
  # Cleanup - remove raw parsed data to save storage
  if [parsed] {
    mutate {
      remove_field => ["parsed"]
    }
  }
  
  # GeoIP enrichment for IP addresses
  if [remote_addr] and [remote_addr] !~ /^(10\.|172\.(1[6-9]|2[0-9]|3[0-1])\.|192\.168\.|127\.)/ {
    geoip {
      source => "remote_addr"
      target => "geoip"
    }
  }
  
  # Security: Remove sensitive information
  mutate {
    remove_field => ["password", "token", "secret", "key", "auth"]
  }
}

# ================================== OUTPUTS ==================================

output {
  # Main output to Elasticsearch
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    
    # Dynamic index naming based on log type and date
    index => "%{[@metadata][index_prefix]}-%{+YYYY.MM.dd}"
    
    # Template and ILM settings
    template_name => "%{[@metadata][index_prefix]}"
    template_pattern => "%{[@metadata][index_prefix]}-*"
    
    # Document type (deprecated in ES 7+, but kept for compatibility)
    # document_type => "_doc"
    
    # Connection settings
    timeout => 60
    retry_on_conflict => 5
    
    # Optional authentication (disabled for development)
    # user => "logstash_writer"
    # password => "your_password"
  }
  
  # Debug output (only enable for troubleshooting)
  if [@metadata][debug] == "true" {
    stdout {
      codec => rubydebug {
        metadata => true
      }
    }
  }
  
  # Dead letter queue for failed documents
  # if "_elasticsearch_output_failure" in [tags] {
  #   file {
  #     path => "/var/log/logstash/failed_docs.log"
  #     codec => json_lines
  #   }
  # }
}
