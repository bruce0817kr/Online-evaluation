#!/usr/bin/env python3
"""
ğŸš€ Gemini AI í˜‘ì—… ê¸°ë°˜ ìµœì í™”ëœ í…ŒìŠ¤íŠ¸ ëŸ¬ë„ˆ
ì˜¨ë¼ì¸ í‰ê°€ ì‹œìŠ¤í…œì˜ ëª¨ë“  í…ŒìŠ¤íŠ¸ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì‹¤í–‰í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""

import asyncio
import json
import logging
import multiprocessing
import os
import subprocess
import sys
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('test_runner.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class TestResult:
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    name: str
    status: str  # 'passed', 'failed', 'skipped'
    duration: float
    output: str
    error: Optional[str] = None
    coverage: Optional[float] = None

class OptimizedTestRunner:
    """Gemini AI í”¼ë“œë°± ê¸°ë°˜ ìµœì í™”ëœ í…ŒìŠ¤íŠ¸ ëŸ¬ë„ˆ"""
    
    def __init__(self, project_root: str = None):
        self.project_root = Path(project_root or os.getcwd())
        self.results: List[TestResult] = []
        self.start_time = None
        self.parallel_workers = min(multiprocessing.cpu_count(), 4)
        self.cache_dir = self.project_root / '.test_cache'
        self.cache_dir.mkdir(exist_ok=True)
        
    def run_command(self, command: List[str], cwd: str = None, timeout: int = 300) -> Tuple[int, str, str]:
        """ëª…ë ¹ì–´ ì‹¤í–‰ (íƒ€ì„ì•„ì›ƒ ë° ì—ëŸ¬ ì²˜ë¦¬ í¬í•¨)"""
        try:
            process = subprocess.Popen(
                command,
                cwd=cwd or self.project_root,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                bufsize=1,
                universal_newlines=True
            )
            
            stdout, stderr = process.communicate(timeout=timeout)
            return process.returncode, stdout, stderr
            
        except subprocess.TimeoutExpired:
            process.kill()
            return -1, "", f"Command timed out after {timeout} seconds"
        except Exception as e:
            return -1, "", str(e)

    def check_dependencies(self) -> Dict[str, bool]:
        """í•„ìˆ˜ ì˜ì¡´ì„± í™•ì¸"""
        dependencies = {
            'node': ['node', '--version'],
            'npm': ['npm', '--version'],
            'python': ['python3', '--version'],
            'playwright': ['npx', 'playwright', '--version']
        }
        
        status = {}
        for name, cmd in dependencies.items():
            try:
                returncode, _, _ = self.run_command(cmd, timeout=30)
                status[name] = returncode == 0
                logger.info(f"âœ… {name}: {'Available' if status[name] else 'Not available'}")
            except Exception:
                status[name] = False
                logger.error(f"âŒ {name}: Not available")
                
        return status

    def setup_frontend_environment(self) -> bool:
        """í”„ë¡ íŠ¸ì—”ë“œ í™˜ê²½ ì„¤ì • (ì˜ì¡´ì„± ì„¤ì¹˜)"""
        frontend_dir = self.project_root / 'frontend'
        if not frontend_dir.exists():
            logger.error("Frontend directory not found")
            return False
            
        logger.info("ğŸ”§ Setting up frontend environment...")
        
        # package-lock.jsonê³¼ node_modules ì œê±° (ê¹¨ë—í•œ ì„¤ì¹˜)
        lock_file = frontend_dir / 'package-lock.json'
        node_modules = frontend_dir / 'node_modules'
        
        if lock_file.exists():
            lock_file.unlink()
        if node_modules.exists():
            import shutil
            shutil.rmtree(node_modules, ignore_errors=True)
        
        # ì˜ì¡´ì„± ì„¤ì¹˜ (í˜¸í™˜ì„± ì˜µì…˜ ì ìš©)
        install_cmd = ['npm', 'install', '--legacy-peer-deps', '--no-fund', '--no-audit']
        returncode, stdout, stderr = self.run_command(install_cmd, cwd=frontend_dir, timeout=600)
        
        if returncode != 0:
            logger.error(f"Frontend dependency installation failed: {stderr}")
            return False
            
        logger.info("âœ… Frontend environment ready")
        return True

    def run_frontend_tests(self) -> TestResult:
        """í”„ë¡ íŠ¸ì—”ë“œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        start_time = time.time()
        frontend_dir = self.project_root / 'frontend'
        
        logger.info("ğŸ§ª Running frontend unit tests...")
        
        # Jest í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (CI ëª¨ë“œ)
        test_cmd = ['npm', 'run', 'test:ci']
        returncode, stdout, stderr = self.run_command(test_cmd, cwd=frontend_dir, timeout=300)
        
        duration = time.time() - start_time
        
        if returncode == 0:
            # ì»¤ë²„ë¦¬ì§€ ì •ë³´ ì¶”ì¶œ
            coverage = self.extract_coverage_info(stdout)
            return TestResult(
                name="Frontend Unit Tests",
                status="passed",
                duration=duration,
                output=stdout,
                coverage=coverage
            )
        else:
            return TestResult(
                name="Frontend Unit Tests",
                status="failed",
                duration=duration,
                output=stdout,
                error=stderr
            )

    def run_e2e_tests(self) -> TestResult:
        """E2E í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        start_time = time.time()
        frontend_dir = self.project_root / 'frontend'
        
        logger.info("ğŸ­ Running E2E tests...")
        
        # Playwright ë¸Œë¼ìš°ì € ì„¤ì¹˜ í™•ì¸
        install_cmd = ['npx', 'playwright', 'install', '--with-deps']
        self.run_command(install_cmd, cwd=frontend_dir, timeout=300)
        
        # E2E í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ)
        test_cmd = ['npx', 'playwright', 'test', '--reporter=json']
        returncode, stdout, stderr = self.run_command(test_cmd, cwd=frontend_dir, timeout=600)
        
        duration = time.time() - start_time
        
        if returncode == 0:
            return TestResult(
                name="E2E Tests",
                status="passed",
                duration=duration,
                output=stdout
            )
        else:
            return TestResult(
                name="E2E Tests",
                status="failed",
                duration=duration,
                output=stdout,
                error=stderr
            )

    def run_api_tests(self) -> TestResult:
        """API í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (Newman)"""
        start_time = time.time()
        api_dir = self.project_root / 'tests' / 'api'
        
        logger.info("ğŸ”Œ Running API tests...")
        
        if not (api_dir / 'postman-collection.json').exists():
            return TestResult(
                name="API Tests",
                status="skipped",
                duration=0,
                output="Postman collection not found"
            )
        
        # Newman í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        test_cmd = [
            'newman', 'run', 'postman-collection.json',
            '--reporters', 'json',
            '--reporter-json-export', 'newman-results.json'
        ]
        returncode, stdout, stderr = self.run_command(test_cmd, cwd=api_dir, timeout=300)
        
        duration = time.time() - start_time
        
        if returncode == 0:
            return TestResult(
                name="API Tests",
                status="passed",
                duration=duration,
                output=stdout
            )
        else:
            return TestResult(
                name="API Tests",
                status="failed",
                duration=duration,
                output=stdout,
                error=stderr
            )

    def run_performance_tests(self) -> TestResult:
        """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        start_time = time.time()
        perf_dir = self.project_root / 'tests' / 'performance'
        
        logger.info("âš¡ Running performance tests...")
        
        if not perf_dir.exists():
            return TestResult(
                name="Performance Tests",
                status="skipped",
                duration=0,
                output="Performance test directory not found"
            )
        
        # Lighthouse ê°ì‚¬ ì‹¤í–‰
        test_cmd = ['npx', 'playwright', 'test', 'lighthouse-audit.spec.js']
        returncode, stdout, stderr = self.run_command(test_cmd, cwd=perf_dir, timeout=300)
        
        duration = time.time() - start_time
        
        if returncode == 0:
            return TestResult(
                name="Performance Tests",
                status="passed",
                duration=duration,
                output=stdout
            )
        else:
            return TestResult(
                name="Performance Tests",
                status="failed",
                duration=duration,
                output=stdout,
                error=stderr
            )

    def extract_coverage_info(self, output: str) -> Optional[float]:
        """í…ŒìŠ¤íŠ¸ ì¶œë ¥ì—ì„œ ì»¤ë²„ë¦¬ì§€ ì •ë³´ ì¶”ì¶œ"""
        try:
            lines = output.split('\n')
            for line in lines:
                if 'All files' in line and '%' in line:
                    # Jest ì»¤ë²„ë¦¬ì§€ ë¼ì¸ì—ì„œ í¼ì„¼íŠ¸ ì¶”ì¶œ
                    parts = line.split()
                    for part in parts:
                        if part.endswith('%'):
                            return float(part.replace('%', ''))
        except Exception:
            pass
        return None

    def run_parallel_tests(self) -> List[TestResult]:
        """ë³‘ë ¬ë¡œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        test_functions = [
            self.run_frontend_tests,
            self.run_e2e_tests,
            self.run_api_tests,
            self.run_performance_tests
        ]
        
        results = []
        with ThreadPoolExecutor(max_workers=self.parallel_workers) as executor:
            # í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰
            future_to_test = {executor.submit(test_func): test_func.__name__ 
                             for test_func in test_functions}
            
            for future in as_completed(future_to_test):
                test_name = future_to_test[future]
                try:
                    result = future.result()
                    results.append(result)
                    logger.info(f"âœ… {result.name} completed: {result.status}")
                except Exception as exc:
                    logger.error(f"âŒ {test_name} generated an exception: {exc}")
                    results.append(TestResult(
                        name=test_name,
                        status="failed",
                        duration=0,
                        output="",
                        error=str(exc)
                    ))
        
        return results

    def generate_report(self) -> Dict:
        """ì¢…í•© í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ ìƒì„±"""
        total_tests = len(self.results)
        passed_tests = sum(1 for r in self.results if r.status == 'passed')
        failed_tests = sum(1 for r in self.results if r.status == 'failed')
        skipped_tests = sum(1 for r in self.results if r.status == 'skipped')
        
        total_duration = sum(r.duration for r in self.results)
        
        # ì»¤ë²„ë¦¬ì§€ ê³„ì‚°
        coverage_results = [r.coverage for r in self.results if r.coverage is not None]
        avg_coverage = sum(coverage_results) / len(coverage_results) if coverage_results else 0
        
        report = {
            'test_run_id': f'optimized_test_{int(time.time())}',
            'timestamp': datetime.now().isoformat(),
            'summary': {
                'total_tests': total_tests,
                'passed': passed_tests,
                'failed': failed_tests,
                'skipped': skipped_tests,
                'success_rate': passed_tests / total_tests if total_tests > 0 else 0,
                'total_duration': total_duration,
                'average_coverage': avg_coverage
            },
            'results': [
                {
                    'name': r.name,
                    'status': r.status,
                    'duration': r.duration,
                    'coverage': r.coverage,
                    'error': r.error
                }
                for r in self.results
            ],
            'gemini_recommendations': self.generate_gemini_recommendations(),
            'performance_metrics': self.calculate_performance_metrics()
        }
        
        return report

    def generate_gemini_recommendations(self) -> List[str]:
        """Gemini AI ìŠ¤íƒ€ì¼ ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ ë¶„ì„
        failed_tests = [r for r in self.results if r.status == 'failed']
        if failed_tests:
            recommendations.append("ğŸ”§ ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ë“¤ì„ ìš°ì„ ì ìœ¼ë¡œ ìˆ˜ì •í•˜ì—¬ ì•ˆì •ì„±ì„ í–¥ìƒì‹œí‚¤ì„¸ìš”.")
        
        # ì»¤ë²„ë¦¬ì§€ ë¶„ì„
        coverage_results = [r.coverage for r in self.results if r.coverage is not None]
        if coverage_results:
            avg_coverage = sum(coverage_results) / len(coverage_results)
            if avg_coverage < 80:
                recommendations.append(f"ğŸ“ˆ í˜„ì¬ ì½”ë“œ ì»¤ë²„ë¦¬ì§€ê°€ {avg_coverage:.1f}%ì…ë‹ˆë‹¤. 80% ì´ìƒì„ ëª©í‘œë¡œ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
        
        # ì„±ëŠ¥ ë¶„ì„
        total_duration = sum(r.duration for r in self.results)
        if total_duration > 600:  # 10ë¶„ ì´ìƒ
            recommendations.append("âš¡ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹œê°„ì´ ê¸¸ì–´ ë³‘ë ¬í™”ë‚˜ ìºì‹±ì„ í†µí•œ ìµœì í™”ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.")
        
        # ìŠ¤í‚µëœ í…ŒìŠ¤íŠ¸ ë¶„ì„
        skipped_tests = [r for r in self.results if r.status == 'skipped']
        if skipped_tests:
            recommendations.append("âš ï¸ ìŠ¤í‚µëœ í…ŒìŠ¤íŠ¸ë“¤ì„ í™œì„±í™”í•˜ì—¬ ë” í¬ê´„ì ì¸ í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ë¥¼ ë‹¬ì„±í•˜ì„¸ìš”.")
        
        return recommendations

    def calculate_performance_metrics(self) -> Dict:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        if not self.results:
            return {}
        
        durations = [r.duration for r in self.results if r.duration > 0]
        
        return {
            'total_execution_time': sum(durations),
            'average_test_time': sum(durations) / len(durations) if durations else 0,
            'fastest_test': min(durations) if durations else 0,
            'slowest_test': max(durations) if durations else 0,
            'parallel_efficiency': self.parallel_workers / max(1, len(durations)) if durations else 0
        }

    def save_report(self, report: Dict) -> str:
        """ë³´ê³ ì„œë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f'gemini_optimized_test_report_{timestamp}.json'
        filepath = self.project_root / filename
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ“„ Test report saved to: {filepath}")
        return str(filepath)

    def run_all_tests(self) -> Dict:
        """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë° ë³´ê³ ì„œ ìƒì„±"""
        self.start_time = time.time()
        
        logger.info("ğŸš€ Starting Gemini AI Optimized Test Run...")
        logger.info(f"ğŸ”§ Using {self.parallel_workers} parallel workers")
        
        # ì˜ì¡´ì„± í™•ì¸
        deps = self.check_dependencies()
        if not all(deps.values()):
            logger.warning("âš ï¸ Some dependencies are missing. Tests may fail.")
        
        # í”„ë¡ íŠ¸ì—”ë“œ í™˜ê²½ ì„¤ì •
        if not self.setup_frontend_environment():
            logger.error("âŒ Frontend environment setup failed")
            return {}
        
        # ë³‘ë ¬ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        self.results = self.run_parallel_tests()
        
        # ë³´ê³ ì„œ ìƒì„±
        report = self.generate_report()
        
        # ë³´ê³ ì„œ ì €ì¥
        report_path = self.save_report(report)
        
        # ê²°ê³¼ ì¶œë ¥
        self.print_summary(report)
        
        return report

    def print_summary(self, report: Dict):
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        summary = report['summary']
        
        print("\n" + "="*80)
        print("ğŸ¯ GEMINI AI ìµœì í™” í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
        print("="*80)
        print(f"ğŸ“Š ì´ í…ŒìŠ¤íŠ¸: {summary['total_tests']}")
        print(f"âœ… ì„±ê³µ: {summary['passed']}")
        print(f"âŒ ì‹¤íŒ¨: {summary['failed']}")
        print(f"â­ï¸  ìŠ¤í‚µ: {summary['skipped']}")
        print(f"ğŸ“ˆ ì„±ê³µë¥ : {summary['success_rate']:.1%}")
        print(f"â±ï¸  ì´ ì‹¤í–‰ ì‹œê°„: {summary['total_duration']:.1f}ì´ˆ")
        print(f"ğŸ“‹ í‰ê·  ì»¤ë²„ë¦¬ì§€: {summary['average_coverage']:.1f}%")
        
        # Gemini ê¶Œì¥ì‚¬í•­ ì¶œë ¥
        if report.get('gemini_recommendations'):
            print("\nğŸ¤– Gemini AI ê°œì„  ê¶Œì¥ì‚¬í•­:")
            for i, rec in enumerate(report['gemini_recommendations'], 1):
                print(f"   {i}. {rec}")
        
        print("="*80)

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Gemini AI ìµœì í™” í…ŒìŠ¤íŠ¸ ëŸ¬ë„ˆ')
    parser.add_argument('--project-root', help='í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬')
    parser.add_argument('--workers', type=int, help='ë³‘ë ¬ ì›Œì»¤ ìˆ˜')
    parser.add_argument('--verbose', '-v', action='store_true', help='ìƒì„¸ ì¶œë ¥')
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    runner = OptimizedTestRunner(args.project_root)
    
    if args.workers:
        runner.parallel_workers = args.workers
    
    try:
        report = runner.run_all_tests()
        
        # ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œëœ ê²½ìš° ì¢…ë£Œ ì½”ë“œ 0
        failed_tests = sum(1 for r in runner.results if r.status == 'failed')
        sys.exit(0 if failed_tests == 0 else 1)
        
    except KeyboardInterrupt:
        logger.info("ğŸ›‘ Test run interrupted by user")
        sys.exit(130)
    except Exception as e:
        logger.error(f"ğŸ’¥ Unexpected error: {e}")
        sys.exit(1)

if __name__ == '__main__':
    main()